% vim: set textwidth=78 fo+=t :

\documentclass[11pt,letterpaper]{article}
\usepackage{acl2013}
\usepackage{amsmath, amsthm}
\usepackage{times}
\usepackage{latexsym}
\usepackage{xspace}
\usepackage{natbib}
\usepackage{tikz-dependency}
\usepackage{placeins}
\usepackage{xcolor}
% Alter some LaTeX defaults for better treatment of figures:
    % See p.105 of "TeX Unbound" for suggested values.
    % See pp. 199-200 of Lamport's "LaTeX" book for details.
    %   General parameters, for ALL pages:
    \renewcommand{\topfraction}{0.9}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{0.8}	% max fraction of floats at bottom
    %   Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     % 2 may work better
    \setcounter{dbltopnumber}{2}    % for 2-column pages
    \renewcommand{\dbltopfraction}{0.9}	% fit big float above 2-col. text
    \renewcommand{\textfraction}{0.07}	% allow minimal text w. figs
    %   Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.7}	% require fuller float pages
	% N.B.: floatpagefraction MUST be less than topfraction !!
    \renewcommand{\dblfloatpagefraction}{0.7}	% require fuller float pages

	% remember to use [htp] or [htpb] for placement


\setlength\titlebox{6.5cm}    % Expanding the titlebox


\renewcommand{\tabcolsep}{5pt}

\newcommand{\baseacc}{00.00\xspace}
\newcommand{\sysacc}{00.00\xspace}
\newcommand{\sysimprove}{00.00\xspace}
\newcommand{\las}{\textsc{las}\xspace}
\newcommand{\uas}{\textsc{uas}\xspace}
\newcommand{\pp}{\textsc{pp}\xspace}
\newcommand{\pos}{\textsc{pos}\xspace}
\newcommand{\wsj}{\textsc{wsj}\xspace}

\newcommand{\stacktop}{S$_0$\xspace}
\newcommand{\buffone}{N$_0$\xspace}

\newcommand{\tuple}[1]{$\langle#1\rangle$}
\newcommand{\maybe}[1]{\textcolor{gray}{#1}}
\newcommand{\note}[1]{\textcolor{red}{#1}}
\newcommand{\state}{\mathcal{S}}

\title{A Non-Monotonic Arc-Eager Transition System for Dependency Parsing}

\author{Author 1\\
	    XYZ Company\\
	    111 Anywhere Street\\
	    Mytown, NY 10000, USA\\
	    {\tt author1@xyz.org}
	  \And
	Author 2\\
  	ABC University\\
  	900 Main Street\\
  	Ourcity, PQ, Canada A1A 1T2\\
  {\tt author2@abc.ca}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
    Previous incremental parsers have assumed that state transitions
    should be monotonic. However, transitions can be made to revise
    previous decisions quite naturally, based on further information.
    Constraining this behaviour amounts
    to placing less trust in the model, which we can expect to be less productive as
    our models improve.

    We show how simple adjustments to the Arc-Eager transition system to relax the
    monotonicity constraint improves accuracy, especially when imperfect
    states are introduced during training. To support our claim that better models
    profit more from non-monotonic transitions, we present a set of
    additional features that improve accuracy substantially. Despite having fewer errors
    to correct, relaxing the monotonicity constraints is even more productive on
    the improved model, for a total improvement on the previous state-of-the-art of
    Y\%.


    %Previous algorithms for incremental parsing have assumed that state transititions
    %should be monotonic. However, psycholinguistic models of sentence processing
    %typically include moves that restructure the parse tree, in order to account
    %for eye-tracking and reading time experiments.

    %We present a non-monotonic transition system
    %that achieves state-of-the-art accuracy for greedy English dependency parsing.
    %We show that simply removing standard constraints on the Arc-Eager system
    %leads to useful non-monotonic behaviour, especially when imperfect states
    %are introduced during training. With the addition of a new transition
    %to correct attachment errors, the repair moves improve accuracy by 0.5\% on
    %the standard evaluation, over a state-of-the-art baseline.
\end{abstract}

% P1
\section{Introduction}
\note{Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus dapibus. Phasellus elementum eleifend neque. Pellentesque non enim urna, in molestie nisi. Sed mattis orci a massa ultricies congue. Suspendisse euismod cursus dolor, a luctus elit ultrices et. Praesent at eros orci. Integer suscipit neque sit amet felis eleifend vel ullamcorper mauris congue. Sed imperdiet ultricies elit in adipiscing. Quisque condimentum consequat turpis at feugiat. Sed fringilla viverra quam, sed auctor nunc convallis vitae. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Phasellus vitae elit vel quam pulvinar cursus ut nec erat.}

\note{Maecenas sed sollicitudin nisl. Maecenas vehicula lacinia tristique. In nec sapien augue, eu tincidunt arcu. Nulla molestie lectus a sapien tincidunt a volutpat nisl consequat. Vivamus consectetur semper risus, nec pretium felis sagittis et. Nam tellus dolor, pulvinar non fermentum quis, auctor a metus. Proin mi purus, dignissim eget tempus in, vestibulum eu libero.}

\note{Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed. Aliquam et augue est, et pretium est. Aenean mattis lacinia elit quis accumsan.}

\note{Nunc pellentesque magna et augue hendrerit dignissim. Donec tempus velit quis purus tincidunt placerat. Suspendisse venenatis aliquet elit id convallis. Praesent porttitor, libero eu pretium gravida, urna velit sollicitudin lectus, sit amet malesuada velit quam tristique diam.}

% P2
% 2-col Dependency parse graphic
% Col 1
%\begin{table*}[ht]
%\centering
%    \begin{tabular}{ll|l}
%Transition & & Precondition \\
%\hline \hline
%Left-Arc   & (notation) & (S0 has no head) \\ 
%Right-Arc  & (notation) &   \\
%Reduce     & (notation) & (S0 has a head) \\  
%Shift      & (notation) & \\
%\end{tabular}
%\caption{In the Arc-Eager transition system, an arc is created either when the word is
%    pushed or popped from the stack. There are thus two pairs of operations: (Shift, Left)
%    and (Right, Reduce). The choice of pop move is constrained by which push move
%    was selected, so that the state is updated monotonically.\\
%Instead, we give the model free choice of pop moves, and reverse the previous
%decision if necessary. This allows the parser to recover from mistakes.}
%\label{tab:transitions}
%\end{table*}


\section{Arc-Eager Transition-based Dependency Parsing}

In transition-based parsing, the parser consists of a state (or a
configuration) which is manipulated by a set of actions.  An action is
applied to a state and results in a new state.  The parsing process
concludes when the parser reaches a final state, at which case the
parse tree is read off of the state.  A particular set of states,
actions and their semantics, yield a transition-system. Our starting
point in this paper is the popular Arc-Eager transition system which
we briefly describe below.  For further details, consult
\citet{nivre:04,nivre-class}.

The state (or configuration) of the arc-eager system is composed of a
stack, a buffer and a set of arcs.

The stack and the buffer hold (indices to) the words of a sentence,
and the set of arcs represent derived dependency relations.

We use a notation in which the stack items are indicated by $S_i$,
with $S_0$ being the top of the stack, $S_1$ the item previous to it
and so on.  Similarly, the buffer items are indicated as $B_i$, with
$B_0$ being the first item on the buffer.  The arcs are of them form
$(h,l,m)$, indicating a dependency arc in which the word $m$ modifies
the word $h$ with a dependency label $l$.

The initial configuration is initialized with an empty stack, and a
buffer containing the words of the sentence in order, followed by an
ROOT token\footnote{\citet{nivre:squib} has shown that setting the ROOT
token at the end is usually superior to placing it at the beginning, as it is easier
to determine root attachments at the end of the sentence.}
In the final configuration the buffer is empty and the stack contains
a single ROOT token. The set of arcs in the final configuration is the
parse tree of the sentence.


There are four parsing actions (Shift, Left, Right, Reduce) that
manipulate stack and buffer items.  The \textbf{Shift} action pops the
first item from the buffer and pushes it on the stack (the Shift
action has a natural precondition that the buffer is not empty).  The
\textbf{Right} action is similar to the Shift action, but it also adds
a dependency arc $(S_0, B_0)$
with the current top of the stack as the head of the newly pushed item
(the Right action has an additionalprecondition that the stack is not
empty).\footnote{%
For labelled dependency parsing, the Right and Left actions are
parameterized by a label $L$ such that the action $Right_L$ adds an
arc $(S_0, L, B_0)$, similarly for $Left_L$.}
The \textbf{Left} action adds a dependency arc $(B_0, H_0)$ with the
first item in the buffer as the head of the top of the stack, and pops
the stack (with a precondition that the stack and buffer are not
empty, and that $S_0$ is not assigned a head yet). Finally, the
\textbf{Reduce} action pops the stack, with a precondition that the
stack is not empty and that $S_0$ is already assigned a head.

\subsection{Monotonicty}

The preconditions of the Left and Reduce actions ensure that once an
arc is added it is never removed in a subsequent state (alternatively,
once a word is assinged a head it cannot be assigned a different head
later on), and that words cannot be removed from the stack before they
are assigned a head. We refer to these properties as the
\textit{monotonicity} of the system.

Due to monotonicuty, there is a natural pairing between the Right and
Reduce actions and the Shift and Left actions: a word which is pushed
into the stack by the Right action must be popped using a Reduce
action, and a word which is pushed by the Shift action must be popped
using the Left action.
In what follows, we break this pairing and suggest a non-monotonic
variant of the Arc-Eager transition system.


%A transition-based dependency parser approaches the task of building a dependency
%graph as a series of \emph{state transitions}, typically from start-to-finish
%along the sentence. A \emph{transition system} is the set of operations that the
%parser has available. We restrict our attention to the Arc-Eager system \citep{nivre:04}
%for projective dependencies.
%
%The \emph{state} can be represented by the
%4-tuple $(H, L, B, S)$, for the Heads, Labels, Buffer, and Stack. The Heads
%and Labels arrays are sufficient for storing the parse. When the head of word $i$
%is set to token $k$ with the label $l$, we will set $H[i]=k$ and $L[i]=l$.
%The buffer is simply a queue of the words yet to be processed in the sentence.
%We assume that we begin parsing with the first word of the sentence on the stack.
%
%It is easier to intuit the system if we begin with a subset. The Right-Arc move
%sets $H[N_0]=S_0$, and pushes $B_0$ to the stack. The former $S_0$ is now $S_1$,
%and $N_1$ is now $N_0$, as we advance a token in the buffer. 
%With only the Right-Arc, we will build a unary tree of depth $n$ for every
%sentence of length $n$.
%At the $i$th word of the sentence, there will be $i$ words on the stack, and
%every word's head will be the word before it in the string, and also immediately
%above it on the stack.
%
%To pop the stack, we add the Reduce move, which makes no other action.
%It does not create any dependencies or advance the buffer.
%When a word is on top of the stack, it is at the tip of the branch being built.
%Applying the Reduce move corresponds to walking up the branch one node, making the
%parent the active attachment point. Consider the tree shape that would result
%from alternating between the moves.
%Every word would be be children of the first word, and siblings to each other.
%
%Because we cannot Reduce an empty stack or Right-Arc into an empty buffer,
%the (Right, Reduce) transition system processes a sentence of length $N$ in exactly $2N$
%transitions, with exactly $N$ Right-Arcs and exactly $N$ Reduce moves. Furthermore,
%every word $w$ there will be exactly one Right-Arc such that $N_0=w$, and exactly
%one Reduce such that $S_0=w$. Each word must be `processed' by both moves; it must be
%pushed with the Right-Arc and popped with the Reduce.
%
%The same is true for a (Left, Shift) system. A good way to think of the two
%systems is that they are really the same, except that one move in each system
%writes a dependency as a side-effect. Viewed this way, it is natural to view the
%Left-Arc as a slightly different Reduce, and the Right-Arc as a slightly different Shift.
%
%Viewed this way, it is easy to see that there will still be exactly $2N$ transitions
%per sentence, although now there are two ways a word might be pushed, and two ways
%it might be popped. However, there are only two pairings that have exactly one
%arc-creating move, (Right, Reduce) and (Left, Shift). These are the only valid
%pairings in the Arc-Eager system.
%
%This paper is about forming dependency trees out of move histories that include
%the (Right, Left) and (Shift, Reduce) pairings. The Arc-Eager system adds
%constraints to rule these transition histories out. We now describe our alternate
%solution.

\section{The Non-Monotonic Arc-Eager Transition System}

The Arc-Eager transition system \citep{nivre:04} has four moves. Two create 
dependencies, two push the first word of the buffer to the stack, and two pop 
the stack:

\begin{center}
    \begin{tabular}{l|cc}
             & Push  & Pop    \\
           \hline
           Adds head   & \emph{Right} & \textbf{Left}    \\
            No head    & \textbf{Shift} & \emph{Reduce}   \\
     \end{tabular}
\end{center}

Every word in the sentence is pushed once and popped once; and every
word must have exactly one head. This creates two pairings, along the
diagonals: (S, L) and (R, D).
Either the push move adds the head or the pop move does, but not both and not neither.
The Arc-Eager system guarantees this by constraining the choice of pop move:

\begin{itemize}\setlength{\itemsep}{-2mm}
    \item Iff $S_0$ has a head, Reduce;
    \item Iff $S_0$ has no head, Left-Arc.
\end{itemize}
In the existing solution, the first move decides both.
Our idea is to let the pair be decided by the \emph{second} move.

This makes the transition system \emph{non-monotonic}, because if the model decides
on an incongruent pairing
we will have to either undo or add a
dependency, depending on whether we correct a Right-Arc with a Left-Arc, or correct
a Shift with a Reduce. We now describe these operations.

\begin{figure}
    \centering
    \begin{dependency}[theme=simple]
        \tikzstyle{t}=[text=green,ultra thick,font=\bfseries\itshape]
        \tikzstyle{f}=[text=red,ultra thick,font=\bfseries\itshape]
        \tikzstyle{m}=[font=\bfseries\itshape]
        \tikzstyle{n}=[font=\itshape]

        \begin{deptext}[column sep=.075cm, row sep=.1ex]
            I \&           \& saw \&          \& Jack       \& \& and     \&           \& Jill   \&  \& fall \& \textsc{r} \\
|[m]|S \& |[m]|L \& |[m]|S   \& |[f]|R \& |[m]|R \& |[m]|D \& |[m]|R \& |[m]|D \& |[t]|L \& |[n]|R \& |[n]|D \& |[n]|L \\
            1 \&     2       \& 3  \&   4      \& 5          \& 6 \& 7     \& 8 \& 9 \& 10 \& 11 \& 12 \\
    I \&           \& saw \&          \& Jack       \& \& and     \&           \& Jill   \&      \& fall \& \textsc{r} \\
        \end{deptext}
    \depedge{3}{1}{2}
    \depedge[red, ultra thick]{3}{5}{4}
    \depedge{5}{7}{5}
    \depedge{5}{9}{7}
    
    \deproot[edge height=0.7cm, ultra thick]{11}{}
    \wordgroup{1}{3}{3}{}
    \wordgroup{1}{5}{5}{}
    
    \depedge[edge below]{3}{1}{2}
    \depedge[edge below]{5}{7}{5}
    \depedge[edge below]{5}{9}{7}
    \depedge[edge below, green, ultra thick]{11}{5}{9}
    \wordgroup{4}{3}{3}{}
\end{dependency}
\caption{
    \small
    State before and after a non-monotonic Left-Arc.
    At move 9, \emph{fall} is the first word of the buffer (marked with an arrow),
    and \emph{saw} and \emph{Jack} are on the stack (circled). The arc created at move 4 was
    incorrect (in red). Arcs are labelled with the move that created them.
    After move 9 (the lower state), the non-monotonic Left-Arc move
    has replaced the incorrect dependency with a correct Left-Arc (in green).
\label{fig:clobber}}
\end{figure}


\subsection{Non-monotonic Left-Arc}

The upper arcs in Figure \ref{fig:clobber} show a transition history where the wrong
push move was selected. The parser began correctly by Shifting \emph{I}
and Left-Arcing it to \emph{saw}, which was then also Shifted. The incorrect move,
4 (in red), is to Right-Arc \emph{Jack} instead of Shifting it.

The difficulty of this kind of sentence for an incremental parser is fundamental.
The leftward context does not constrain the decision, 
and an arbitrary amount of text could separate \emph{Jack} from its true head,
\emph{fall}. Eye-tracking
experiments show that humans often perform a saccade while reading such examples (TODO).

In moves 5-8 the parser correctly builds the rest of the \textsc{np}, and arrives
at \emph{fall}. The monotonicity constraints would here force an incorrect analysis,
even though the evidence for the correct decision is much stronger at the later move.

We allow Left-Arcs to `clobber' Right-Arcs if the model
recommends it. The Right-Arc is deleted, and the Left-Arc proceeds as normal. The
effect of this is exactly as if the model had correctly chosen Shift at
move 4. We simply give the model a second chance to make the correct choice.

\subsection{Non-monotonic Reduce}

The upper arcs in Figure \ref{fig:adduce} show a state resulting from the opposite error.
The parser has Shifted \emph{Jack} instead of Right-Arcing it. After
building the \textsc{np} the buffer is exhausted, except for the artificial root token,
which we use to Left-Arc the head of the sentence, following \citet{nivre:squib}.

Instead of letting the previous choice lock us in to the pair (Shift, Left), we let
the latter decision reverse it to (Right, Reduce). This gives the model the flexibility
to take advantage of the extra information, and make what should be an easy decision.
When the Shift/Right decision is reversed, we add an arc between the top of the stack
and the word above it. This is the arc that would have been created had the parser
chosen to Right-Arc when it chose to Shift. Since our idea is to reverse this mistake,
we select the Right-Arc label that the model scored most highly at that time.


\begin{figure}
    \centering
    \begin{dependency}[theme=simple]
    \tikzstyle{t}=[text=green,ultra thick,font=\bfseries\itshape]
    \tikzstyle{f}=[text=red,ultra thick,font=\bfseries\itshape]
    \tikzstyle{m}=[font=\bfseries\itshape]
    \tikzstyle{n}=[font=\itshape]
    \begin{deptext}[column sep=.075cm, row sep=.1ex]
        I \&           \& saw \&          \& Jack       \& \& and     \&           \& Jill \&   \& \textsc{r} \\
       |[m]|S \& |[m]|L \& |[m]|S   \& |[f]|S \& |[m]|R \& |[m]|D \& |[m]|R \& |[m]|D \& |[m]|R \& |[m]|D \& |[t]|D \& |[n]|L \\
            1 \&     2       \& 3  \&   4      \& 5          \& 6 \& 7     \& 8 \& 9 \& 10 \& 11 \& 12 \\
            I \&           \& saw \&          \& Jack       \& \& and     \&           \& Jill \& \& \textsc{r} \\
\end{deptext}
    \depedge{3}{1}{2}
    \depedge{5}{7}{5}
    \depedge{5}{9}{7}
    
    \deproot[edge height=0.7cm, ultra thick]{11}{}
    \wordgroup{1}{3}{3}{}
    \wordgroup{1}{5}{5}{}
    
    \depedge[edge below]{3}{1}{2}
    \depedge[edge below]{5}{7}{5}
    \depedge[edge below]{5}{9}{7}
    \depedge[edge below, green, ultra thick]{3}{5}{9}
    \wordgroup{4}{3}{3}{}
\end{dependency}
\caption{
\small
State before and after a non-monotonic Reduce.
After making the wrong push move at 4, at move 11
the parser has \emph{Jack} on the stack (circled), with only the dummy \textsc{root}
token left in the buffer. A monotonic parser must deterministically Left-Arc
\emph{Jack} here to preserve the previous decision, despite the current state.
We remove this constraint, and instead assume that when the model selects Reduce
for a headless item, it is reversing the previous Shift/Right decision. We add
the appropriate arc, assigning the label that scored highest when the Shift/Right
decision was originally made.
\label{fig:adduce}}
\end{figure}


\subsection{Why have two push moves?}

We have argued above that it is better to trust the second decision that the model
makes, rather than using the first decision to determine the second. If this is
the case, is the first decision entirely redundant?
Instead of defining how pop moves can correct Shift/Right mistakes, we could
instead eliminate the ambiguity. There are two possibilities:
Shift every token, and create all Right-Arcs via Reduce; or Right-Arc every token,
and override the incorrect arcs with subsequent Left-Arcs.

The problem with these approaches is that it is useful to condition
on the results of previous decisions. Saving all of the difficulty for later
is not a very good structured prediction strategy. Most of the transition decisions are
easy, and when the parser does encounter a difficult decision, it is advantageous
to have the dependency structure as a summary of the previous information, in
much the same way that the previous tags are critical for \pos tagging. 

As an example of the problem, if the Shift move is
eliminated, about half of the Right-Arcs created will be spurious. All of these
arcs will be assigned labels\footnote{If the parser is trained to label the temporary
arcs differently, its action is roughly (but not fully) isomorphic to the Shift move.},
making important features uselessly noisy.

The non-monotonic transition system that we propose does not have this problem.
The model makes Shift vs. Right decisions as normal, and conditions on them --- but
without \emph{committing} to them.



\section{Dynamic oracle for the Arc-Eager system}
\label{ref:oracle}

This section describes the previous work of \citet{goldberg:12} in describing
a \emph{dynamic oracle} for the Arc-Eager transition system.
We opt for a different presentation of the oracle,
to make its relationship with the new oracle for the non-monotonic transition
system clearer.

The key novel utility of this oracle is that it
$O$ is defined for \emph{any} state. The previous oracle, described by 
\citet{nivre:04}, was only defined for states resulting from gold-standard
transitions. The old approach is analogous to training a \pos tagger
and conditioning on gold-standard tags for the preceding words.
Non-monotonic transitions are useless in such a set-up, as examples of their
application are never encountered during training. We therefore need a version
of the oracle correct for our adjusted transition system.

A \emph{dynamic oracle} is a function
from a parser state $\state$ and a proposed transition, $T$, to a cost, $c$:

\begin{equation}
c = O(\state, T)
\end{equation}

The cost denotes the number of gold-standandard arcs that cannot be
recovered once the transition has been applied. This can be restated as:

\begin{equation}
c = O(\state) - O(T(\state))
\end{equation}

With this formulation, we need not define the oracle in terms of the individual
transitions, as \citet{goldberg:12} do. That step is important when implementing
the oracle, and perhaps makes its action easier to intuit,
but is not necessary for the present purpose.

The way forward to calculating $O$ is through a
convenient property of the 
Arc-Eager system, proven by TODO.  If a
set of arcs which can be extended to a projective tree can be individually
derived from a given configuration, then a projective tree containing all of
the arcs in the set is also derivable from the same configuration.

The next step, then, is to define what arcs can be reached from state $\state$.
It is then trivial to take the intersection of this set and the gold-standard
arcs, and the \emph{arc reachability} property guarantees that there will be a
projective tree covering that subset.

For a state with a stack of depth $n$, a buffer of length $i$, and an array of
existing arcs $H$, all and only the arcs:

\begin{eqnarray}
    \langle S_{0...n}, B_{0...i}\rangle \label{eqn:oracle1} \\
    \langle B_{0...i}, S_{i \in {0...n}\;\mbox{if}\; H[S_i] = \emptyset } \rangle \label{eqn:oracle2}\\
    \langle B_{h \in {0...i}}, B_{c \in {0...i}}\;\; \mbox{where}\;\; h \neq c
\end{eqnarray}

Are reachable using the non-monotonic Arc-Eager transitions.
To prove \ref{eqn:oracle1}, let $S_0=w$, and $B_0=k$. The
arc $\langle w, k\rangle$ can be recoverd by the Right-Arc transition, which
will change the state such that $S_0=k$, $S_1=w$ and $B_0=k+1$. The Reduce
move will then change the state such that $S_0=w$, $B_0=k+1$. All arcs
$\langle S_0, B_{0...j}\rangle$ are thus recoverable. To instead recover the arc
$\langle S_1, B_{0}\rangle$, $S_0$ must be popped. This can either be done
with the Reduce move, iff $H[S_0] \neq \emptyset$, or with the Left-Arc otherwise.

For \ref{eqn:oracle2}, recall that the Arc-Eager system constrains
Left-Arcs to prevent arcs where $H[S_0] \neq \emptyset$. The opposite constraint
applies to the Reduce move, so the position of a word on the stack does not
affect whether it can be attached to $B_0$. For the words $B_1...i$, we can
advance them to $B_0$ by Right-Arcing and Reducing each word before them,
after which the former proof applies.

Finally, it is obvious that any dependency between two words in the buffer will
be reachable, while all dependencies between two words on the stack are unreachable,
as are dependencies involving a word neither on the stack nor in the buffer. The
possibilities are thus exhausted.

To recap, some subset of arcs in the gold projective tree will be recoverable
from a given state, which means a tree containing those arcs is also recoverable.
If a transition cuts off at least one previously reachable gold dependency, it is
suboptimal; $c \ge 1$.
At least one transition must be zero cost: for an arc to be reachable, there
must be some transition which is the first step towards reaching it.

\section{Dynamic oracle for the \textsc{nmae} transition system}

Using the above definition of the Arc-Eager dynamic oracle, we can now
define which arcs are reachable when the non-monotonic Left-Arc and Reduce moves
are allowed. They will be the arcs such that:

\begin{eqnarray}
    \langle S_{0...n}, B_{0...i}\rangle \label{eqn:nmo1} \\
    \langle B_{0...i}, S_{0...i} \rangle \label{eqn:nmo2} \\
    \langle B_{h \in {0...i}}, B_{c \in {0...i}}\;\; \mbox{where}\;\; h \neq c\\
    \langle S_1, S_0 \rangle \;\; \mbox{if} H[S_0] = S_1
\end{eqnarray}

We remove the reference to the head constraint for the Left-Arc, and add the
possibility to attach a headless $S_0$ to $S_1$. The weakness of this oracle
for training a parser is discussed in Section \ref{sec:whypush}. The Shift and
Right-Arc moves will always be scored identically, as the oracle encodes no
preference for monotonic paths. What we want instead is for the parser to regard
the non-monotonic moves as repair operations, that are available in case of
emergency. We want to train the parser to step as far towards the correct derivation
as possible at each transition.

We achieve this by making the cost sensitive to both oracles, as well as adding a
`bonus' term, $f$, for a transition adding a gold dependency that was previously
unrecoverable:

\begin{eqnarray}
c = c_m + c_{nm}\\
c_m = O_m(\state) - O_m(T(\state))\\
c_{nm} = O_{nm}(\state) - O_{nm}(T(\state))
\end{eqnarray}

In summary, the oracle will penalise a transition if it makes an arc \emph{require}
a non-monotonic move, or if it rules out an arc entirely.

\section{Dynamic oracles}

 An essential component when training a transition-based parser is an oracle
 which, given a gold-standard tree, dictates the sequence of moves a parser
 should make in order to derive it.  Traditionally, these oracles are defined
 as functions from trees to sequences, mapping a gold tree to a single sequence
of actions deriving it, even if more than one sequence of actions derives the
gold tree. We call such oracles \emph{static}.  Recently, Goldberg and Nivre
\shortcite{coling2012} introduced the concept of a \emph{dynamic} oracle, and
presented a concrete oracle for the arc-eager system.  Instead of mapping a
gold tree to a sequence of actions, the dynamic oracle maps a
\tuple{configuration, tree} pair to a \emph{set} of possible parser actions
which are optimal at the given configuration.  More concretely, the dynamic
oracle presented in \cite{coling2012} maps \tuple{action,configuration,tree}
tuples to an integer, indicating the number of gold arcs in $tree$ that can be
derived from $configuration$ by some sequence of actions, but could not be derived
after applying $action$ to the configuration.\footnote{This correctness of
the oracle is based on a property of the arc-eager system, stating that if a
set of arcs which can be extended to a projective tree can be individually
derived from a given configuration, then a projective tree containing all of
the arcs in the set is also derivable from the same configuration.  This same
property holds also for the transition system we present below.} We refer to
this number as the $cost$ of taking action $a$ at configuration $c$. Actions
with a cost of 0 are optimal at the given configuration, and as long as we
follow only optimal actions we are guaranteed to reach the gold tree.
%By definition, there is at least one 0-cost actions at each configuration.
%Thus, for our purposes we only care to know if the cost of an action is zero
%or non-zero. 
\subsection{Encoding Spurious Ambiguity}
\note{I'm not sure where to include this text}
An important benefit of dynamic oracles over static ones is that they do not
commit to a single sequence of actions for a deriving a given tree, but instead
compactly encode all the deriving sequences. A suitable training algorithm
(see Section \ref{training}) can then be used to learn a good sequence of
actions based on the oracle's guidance.  The task of dealing with spurious
ambiguity is thus moved from the definition of the oracle (a manual task) to
the training procedure.

While spurious ambiguity in the arc-eager transition system is fairly limited, 
it is a crucial aspect of our non-monotonic system.  The non-monotonic
actions are repair-operations, undoing previous decisions.  It would be very
hard to manually design a heuristic dictating which arcs to construct just so
that they can be removed later, and which arcs not to construct in the first
place.  The dynamic-oracle allows us to encode both options, and let the
learner chose when to take each course. 

\subsection{Training to recover from mistakes}
\note{Do we need this? here?}
Another benefit of the dynamic-oracle as defined here is that it is well
defined for every parser configuration, not just configurations leading to the
gold tree.  In a configuration which cannot lead to the gold tree, the oracle
encodes the paths leading to the best tree which is still reachable.  As we
know the parser is going to make mistakes, it is useful to let it see
configurations that result from common mistakes, and teach it the optimal way
to proceed from them.


\subsection{Dynamic-oracle for the Arc-Eager System}

The dynamic oracle for the arc-eager system is based on the properties of the
system by which once words are put on the stack they can acquire heads and
dependents only from the buffer, and once they are removed from the stack they
can no longer acquire any heads or dependents. In addition, once a word
acquired a head it can no longer acquire another head.  Given that words that
are removed from the stack or the buffer can not return there later, it
is straightforward to compute the number of gold arcs that are ``lost'' by an
action by considering the effect of the action on the buffer and the stack,
and reasoning accordingly (the notation assumes the configuration $c$ has $b$
as the first item in the buffer $\beta$ and $s$ is the top of the stack
$\sigma$, and $gold$ is a set of arcs. An arc is a triplet $(h,l,m)$
indicating that $m$ modifies $h$ with a label $l$).




\begin{itemize}
   \item $\mathcal{C}(\textsc{Left-Arc}_l; c, gold)$: 
Adding the arc $(b, l, s)$ and popping $s$ from the stack means that $s$ will not be able
to acquire any head or dependents in $\beta$. The cost is therefore the number of arcs 
in $gold$ of the form $(k, l', s)$ or $(s, l', k)$ such that $k \in \beta$.

\item $\mathcal{C}(\textsc{Right-Arc}_l; c, gold)$: 
Adding the arc $(s, l, b)$ and pushing $b$ onto the stack means that $b$ will 
not be able to acquire any head in $\sigma$ or $\beta$, nor any dependents in $\sigma$. 
The cost is therefore the number of arcs in $gold$ of the form $(k, l', b)$,
such that $k \in \sigma$ or $k \in \beta$, 
or of the form $(b, l', k)$ such that $k \in \sigma$ 
% the following is new according to matt's fix:
and $k$ is not assigned a head.

\item $\mathcal{C}(\textsc{Reduce}; c, gold)$: 
Popping $s$ from the stack means that $s$ will not be able to acquire any dependents
in $b|\beta$. The cost is therefore the number of arcs in $gold$ of the form $(s, l', k)$
such that $k \in b|\beta$. \maybe{While it may seem that a gold arc of the form $(k, l, s)$ should be accounted for as well,
note that a gold arc of that form, if it exists, is already accounted for by a previous (erroneous) \textsc{Right-Arc}$_l$ 
transition when $s$ acquired its head.}

\item $\mathcal{C}(\textsc{Shift}; c, gold)$: 
Pushing $b$ onto the stack means that $b$ will not be able to acquire any head
or dependents in $s|\sigma$. The cost is therefore the number of arcs in $gold$ of 
the form $(k, l', b)$ such that $k \in s|\sigma$ and of the form $(b, l', k)$ such that $k \in s|\sigma$
% the following is new according to matt's fix:
and $k$ is not assigned a head.
\note{Matthew, notice that shift and right-arc changed their definition, as
you pointed out they should, by requiring the stack item to not have a head
assigned already. The deltas below are based on this new and correct
definitions.}
\end{itemize}

\subsection{Following non-gold transitions in training}
\begin{figure}
\centering
    \begin{verbatim}

while not state.is_finished {
  feats = extract(state)
  zero_costs = oracle(state)
  g = predict_from(feats, zero_costs)
  monotonic = check_constraints(state)
  p = predict_from(feats, monotonic)
  if p not in zero_costs {
     update(feats, g, 1)
     update(feats, p, -1)
  }
  transition(state, p)
}
\end{verbatim}
\caption{Dynamic oracle. TODO: Notation.}
\end{figure}

\subsection{Defining the oracle}

The new system .. like the old one ..
.. with the following changes:
\begin{itemize}
   \item Once a word has acquired a head to its right it can still acquire a
      different head to its left.  (due to change of the Left-arc transition).
   \item Once words are put on the stack they can acquire heads not only from
      the buffer but \textit{also from the word just before them on the
      stack.} (due to the changed semantics of the reduce transition).
\end{itemize}

We will define the new dynamic oracles $\mathcal{C}_{NML}$,
$\mathcal{C}_{NMD}$ and $\mathcal{C}_{NML+D}$ based on the arc-eager dynamic
oracle $\mathcal{C}$ defined above, with additional $\Delta$ terms:

\noindent$\mathcal{C}_{NML}(a,c,t) = \mathcal{C}(a,c,t) + \Delta_{NML}(a,c,t)$\\

\noindent$\mathcal{C}_{NMD}(a,c,t) = \mathcal{C}(a,c,t) + \Delta_{NMD}(a,c,t)$\\

\noindent$\mathcal{C}_{NML+D}(a,c,t) = \mathcal{C}(a,c,t) + \Delta_{NML}(a,c,t) + \Delta_{NMD}(a,c,t)$\\

The terms $\Delta_{NML}$ and $\Delta_{NMD}$ reflect the score adjustments that
need to be done to the arc-eager oracle due to the changes of the Left-Arc and
Reduce actions, respectively.

The change due to the non-monotonic Left-Arc action:

\begin{itemize}
   \item $\Delta_{NML}(\textsc{RightArc},c,gold)$: The cost of Right-arc is decreased by 1 if the gold head of $b$ is on the buffer
(because $b$ can still acquire its correct head later with a Left-Arc action).
It is increased by 1 for any word $w$ on the stack such that $b$ is the gold
parent of $w$ and $w$ is assigned a head already.

   \item $\Delta_{NML}(\textsc{Reduce},c,gold)$: The cost of Reduce is increased by 1 if the gold head of $s$ is on the buffer
(because this will preclude $s$ from acquiring its correct head later on with
a Left-Arc action).

   \item $\Delta_{NML}(\textsc{LeftArc},c,gold)$: The cost of Left-Arc is increased by 1 if $s$ is already assigned to its gold
parent.

  \item $\Delta_{NML}(\textsc{Shift},c,gold)$: The cost of Shift is increased by 1 for any word $w$ on the stack such that $b$ is the gold
parent of $w$ and $w$ is assigned a head already.
\end{itemize}

The change due to the non-monotonic Reduce action:
\begin{itemize}

   \item $\Delta_{NMD}(\textsc{Shift},c,gold)$: The cost of Shift is decreased by 1 if the gold head of $b$ is $s$.

   \item $\Delta_{NMD}(\textsc{LeftArc},c,gold)$: The cost of Left-Arc is increased by 1 if $s$ is not assigned a head, and the gold head of $s$ is $s_{-1}$.

   \item $\Delta_{NMD}(\textsc{RightArc},c,gold)$ = 0

   \item $\Delta_{NMD}(\textsc{Reduce},c,gold)$ = 0

\end{itemize}

\section{Training Procedure}

Now that we have defined dynamic oracles for the non-monotonic arc-eager
system, we could just plug them in the training algorithm of
\cite{coling2012}.  However, the non monotonic oracles does not give the
parser any incentive to learn not make mistakes that it can potentially recover from
later, even if learning to fix the mistake is harder than learning not to make
it in the first place.

In order to correct for that, we can instead train the parser using both the
monotonic and non-monotonic oracles simultaneously by combining their
judgements: if the non-monotonic oracle assigns several actions a zero-cost,
we prefer to follow those actions that are also assigned a zero-cost by the
monotonic oracle, because these actions can lead to the best outcome without
relying on a repair down the road.

\note{should make nicer!}

% P5

\section{Experiments}

We implemented a standard Arc-Eager parser trained with the averaged Perceptron,
and used it to evaluate the non-monotonic transitions. We tested the transitions on
multiple dependency schemes, to check whether any effects were specific to annotation
contingencies. We also analyse the effect on accuracy by dependency type, and
investigate the impact on error propagation.


\begin{table}[t]

% UAS
% baseline 21: 90.4 +/- 0.10
% reattach 21: 90.2 +/- 0.09
% adduce 21: 90.4 +/- 0.08
% both 21: 90.3 +/- 0.09
% LAS
% baseline 21: 87.8 +/- 0.10
% reattach 21: 87.6 +/- 0.09
% adduce 21: 87.8 +/- 0.08
% both 21: 87.6 +/- 0.09
    \small
    \centering
    \begin{tabular}{l|rrrr}
        \hline
        & \multicolumn{2}{c}{Stanford} \\
        & \textsc{w}  & \textsc{s} \\
\hline \hline
        & \multicolumn{4}{c}{Unlabelled Attachment} \\
        \hline
Baseline & 90.4 & 41.2 \\
NM L & 90.2 & 41.2 \\
NM D & 90.4 & 41.1 \\
NM L+D & 90.3 & 41.2\\
\hline
            & \multicolumn{2}{c}{Labelled Attachment} \\
            \hline
Baseline & 87.8 & 31.4 \\
NM L & 87.6 & 31.3 \\
NM D & 87.8 & 31.3 \\
NM L+D & 87.6 & 31.5 \\
\hline
    \end{tabular}
    \caption{\small Non-monotonic transitions are not useful with
        \textbf{standard training},
where all training examples have gold-standard transition histories. Results refer to
\wsj 22.\label{tab:standard}}
\end{table}


\subsection{Data and Evaluation}

A train/dev/test split of 02-21 of the Penn Treebank \textsc{wsj} \citep{marcus:94}
was used for all models. The data was converted into
Stanford dependencies \citep{stanford_deps} after the \citet{vadas:07}
\textsc{np}-bracketing patch was applied. We also evaluate our models on
dependencies created by the \textsc{Penn2MALT} tool, to assist comparison 
with previous results.

The data was \pos tagged using the tagger described by \citep{zhang_pos:11}.
As is standard, 4-fold jack-knifing was used to prevent learning incorrect
weights from gold-standard tags.
% P6

\subsection{Parsing model}

We base our experiments on the parser described by \citet{goldberg:12}. We
began by implementing their baseline system, a standard Arc-Eager parser using
an averaged Perceptron learner and the extended feature set described by \citet{zhang:11}.
We follow \citeauthor{goldberg:12} in training all models for 15 iterations,
and in shuffling the sentences before each iteration.
The system is sensitive to the ordering of the sentences, so
the order is shuffled before each iteration and
so accuracies are reported as the average of 20 trials. All experiments had a
standard deviation between TODO and TODO\%.

Labels and moves were learned jointly, resulting in a single model with 88 classes
for the Stanford dependencies. Joint label/move learning proved surprisingly
advantageous, giving 0.4\% additional \uas.

We follow the suggestion of \citet{nivre:squib} to handle root-node dependencies by
adding a dummy token at the \emph{end} of the sentence.
This improves accuracy by delaying root-node decisions, and likely accounts for the
0.2\% extra accuracy of our standard-trained baseline system over the equivalent result
reported by \citet{goldberg:12}.

\begin{figure}
\begin{verbatim}

while not state.is_finished {
  feats = extract(state)
  zero_costs =
      O_r(state) intersect O_e(state)
  if zero_costs.empty():
   zero_costs = O_r(state)

  g = predict_from(feats, zero_costs)
  p = predict_from(feats, monotonic)
  if p not in zero_costs {
     update(feats, g, 1)
     update(feats, p, -1)
  }
  transition(state, p)
}
\end{verbatim}
\end{figure}



\subsection{Training strategies}

Section \ref{sec:oracle} summarises a recent innovation in the way transition-based
dependency parsers are trained. This method allows training from states that
result from previous transition errors, which is
important for this work. We therefore evaluate our model with two training strategies.

\textbf{Standard.} 
The parser begins stepping through the sentence, and at each state a small
set of rules determines a single gold-standard transition. The transitions
determine both the label for the instance and the next parser action. No ambiguity
is preserved; the Perceptron's weights are updated even if its prediction was a move
that could lead to the gold-standard dependencies.
This is the baseline method for \citet{goldberg:12}.

\textbf{Goldberg and Nivre.} The parser is allowed to train from states
resulting from previous incorrect transitions. The function determing correctness
is described in Section \ref{sec:oracle}. Another important difference is that
multiple transitions may be regarded as correct, if they do not cause any new
errors. Any prediction within this set will not result in an update to the
Perceptron's weights. This is the method labelled \emph{dynamic+explore}
in \citet{goldberg:12}.

\begin{table}[t]
% Stanford
% UAS
% baseline 21: 91.2 +/- 0.08
% reattach 21: 91.4 +/- 0.07
% adduce 21: 91.4 +/- 0.08
% both 21: 91.6 +/- 0.08
% LAS
% baseline 21: 88.7 +/- 0.06
% reattach 21: 89.0 +/- 0.08
% adduce 21: 88.9 +/- 0.07
% both 21: 89.1 +/- 0.09
    \small
    \centering
    \begin{tabular}{l|rrrr}
        \hline
        & \multicolumn{2}{c}{Stanford} & \multicolumn{2}{c}{MALT}  \\
        & \textsc{w}  & \textsc{s} & \textsc{w} & \textsc{s} \\
        \hline \hline
        & \multicolumn{4}{c}{Unlabelled Attachment} \\
        \hline
        Baseline & 91.2 & 42.0 & 90.9 & 39.7 \\
        NM L & 91.4 & 43.1 & 91.0 & 40.1 \\
        NM D & 91.4 & 42.8 & 91.1 & 41.2 \\
        NM L+D & 91.6 & 43.3 & 91.3 & 41.5 \\
        \hline
        & \multicolumn{4}{c}{Labelled Attachment} \\
        \hline
        Baseline & 88.7 & 31.8 & 89.7 & 36.6 \\
        NM L & 89.0 & 32.5 & 89.8 & 36.9 \\
        NM D & 88.9 & 32.3 & 89.9 & 37.7 \\
        NM L+D & 89.1 & 32.7 & 90.0 & 37.9 \\
        \hline
    \end{tabular}
    \caption{\small
        With \textbf{Goldberg and Nivre training}, both non-monotonic transitions
        bring small improvements in per-token (\textsc{w}) and whole sentence (\textsc{s})
        accuracy on the development data, \wsj 22.
        \label{tab:goldberg}}
\end{table}

\subsection{Transition systems}

\textbf{Baseline.} The unmodified Arc-Eager transition system.

\textbf{NM L.} The pre-condition on Left-Arc is removed, allowing the model to
choose it when the top of the stack already has a head set, which the Left-Arc will
over-ride. This move will be non-monotonic in the
sense that it will delete a previously created dependency, in addition to creating
a Left-Arc as normal.

\textbf{NM D.} The pre-condition on Reduce is removed, allowing the model to choose it
when the top of the stack ($S0$) has no head set, so long as there is another word
above it on the stack ($S1$). $S0$ is then attached as a child of $S1$. This move
is non-monotonic in the sense that it over-rides the previous decision to push $S0$,
instead of Right-Arc it to $S1$. The new arc will be assigned the highest scoring
Right-Arc label from that previous decision.

\textbf{NM L+D.} Both non-monotonic moves are enabled. 

\section{Development Results}
\label{sec:results}

Table \ref{tab:goldberg} shows the effect of the non-monotonic transitions on
labelled and unlabelled attachment score. All results are averages from 20 models
trained with different random seeds, as the ordering of the sentences at each iteration
of the Perceptron algorithm has an effect on the system's accuracy.

The two non-monotonic transitions each bring small 

compares the accuracy of the different transition systems

Table \ref{tab:standard} compares the monotonic (Baseline) and non-monotonic transition
systems when the standard training strategy is used.
but use the standard training strategy for a transition-based parser.
Unsurprisingly, the non-monotonic transitions
were not useful in this system, as the standard training strategy does not include
any examples of states where they are necessary.


%\begin{table}
%\centering
%\small
%\begin{tabular}{lrrrr}
%        & Freq. & Base & NM & WD \\
%    \hline \hline
%    \multicolumn{5}{c}{Left-Arcs} \\
%    \hline
%    det	& 3350 & 00.0 & +0.0 & 0.0 \\
%     nn	& 3208 & 00.0 &	-0.0 & 0.0 \\
%  nsubj	& 2691 & 00.0 &	0.0 & 0.0 \\
%   amod	& 2428 & 00.0 & 0.0 & 0.0 \\
%aux	    & 1229 & 00.0 &	0.0 & 0.0 \\
%advmod  & 810  & 00.0 & 0.0 & 0.0 \\
%num	    & 749  & 00.0 & 0.0 & 0.0 \\
%   poss	& 707  & 00.0 & 0.0 & 0.0 \\
%  Other	& 2637 & 00.0 & 0.0 & 0.0 \\
%\hline
%\multicolumn{5}{c}{Right-Arcs} \\
%\hline
%pobj	& 3739  & 00.0 & 0.0 & 0.0 \\
%prep	& 3498  & 00.0 & 0.0 & 0.0 \\
%dobj	& 1568  & 00.0 & 0.0 & 0.0 \\
%conj	& 1002  & 00.0 & 0.0 & 0.0 \\
%cc	    & 898   & 00.0 & 0.0 & 0.0 \\
%number	& 468   & 00.0 & 0.0 & 0.0 \\
% ccomp	& 449   & 00.0 & 0.0 & 0.0 \\
% xcomp	& 438   & 00.0 & 0.0 & 0.0 \\
%advmod	& 437   & 00.0 & 0.0 & 0.0 \\
%ps	    & 425   & 00.0 & 0.0 & 0.0 \\
%dep	    & 406   & 00.0 & 0.0 & 0.0 \\
%  Other	& 2549  & 00.0 & 0.0 & 0.0 \\
%\hline
%All left  & 000 & 00.0 & 0.0 & 0.0  \\
%All right & 000 & 00.0 & 0.0 & 0.0  \\
%Root    & 1700 & 00.0 & 0.0 & 0.0    \\     
%\hline
%\end{tabular}
%\caption{\small Accuracies for common Stanford labels,
%         using G\&N training and standard features.
%         \textbf{NM L+D} accuracy is shown by difference from the baseline. The
%         WD column shows the difference weighted by the frequency of the label.
%     \label{tab:labels}}
%\end{table}
%

%\subsection{Accuracy by label}



\section{Test Results}

\note{Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.}


\begin{table}
%UAS
%baseline 21: 90.9 +/- 0.04
%both 21: 91.1 +/- 0.07
%LAS
%baseline 21: 88.7 +/- 0.05
%both 21: 88.9 +/- 0.07
% UAS
% baseline 21: 90.6 +/- 0.07
% both 21: 91.0 +/- 0.05
% LAS
% baseline 21: 89.5 +/- 0.07
% both 21: 89.9 +/- 0.06
    \centering
    \small
    \begin{tabular}{l|r|rr|rr}
        \hline 
System  &   $O$ &  \multicolumn{2}{c}{Stanford} & \multicolumn{2}{|c}{Penn2Malt} \\
        &       &  \las  & \uas  & \las & \uas \\
        \hline \hline
K\&C 10  & $n^3$ & ---   & ---   & ---  & 93.00 \\
Z\&N 11  & $nk$  & 91.9  & 93.5  & 91.8 & 92.9 \\
G\&N 12  & $n$   & 88.72 & 90.96 & ---  & --- \\
        \hline
Baseline    & $n$ & 88.7 & 90.9 & 88.7  & 90.6 \\
NM L+D      & $n$ & 88.9 & 91.1 & 88.9  & 91.0 \\
\hline
    \end{tabular}
    \caption{\small Test results on \wsj 23, with comparison against the
        state-of-the-art systems from the literature of different run-times.
        \textbf{K\&C 10}=\citet{koo:10}; \textbf{Z\&N 11}=\citet{zhang:11};
        \textbf{G\&N 12}=\citet{goldberg:12}. The Baseline system is a re-implementation
             of G\&N 12.\label{tab:eval}}
\end{table}

% P8
\section{Related Work}


\note{Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.}

\note{Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.}

\note{Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.}

\subsection{Parse restructuring in psycholinguistics}

\note{Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus.}

\note{Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor.}


\section{Conclusion and future work}

\noindent
We began this paper with the observation that because the Arc-Eager transition system \citep{nivre:04}
attaches a word to its governor either when the word is pushed onto the stack or when it is
popped off the stack, monotonicity (plus the ``tree constraint'' that a word has exactly one governor)
implies that a word's push-move determines its associated pop-move. In this paper we suggest relaxing
the monotonicity constraint to permit the pop-move to alter existing attachments if appropriate,
thus breaking the 1-to-1 correspondence between push-moves and pop-moves.  This permits the parser
to correct some early incorrect attachment decisions later in the parsing process.
Adding additional transitions means
that in general there are multiple transition sequences that generate any given syntactic analysis,
i.e., our non-monotonic transition system generates spurious ambiguities (note that the
Arc-Eager transition system on its own generates spurious ambiguities).
As we explained in the paper, with the greedy best-first search used here additional
spurious ambiguity is
not necessarily a draw-back.

The conventional training procedure for transition-based parsers uses a ``static'' oracle
based on ``gold'' parses that never predicts a non-monotonic transition, so it is clearly not
appropriate here.  Instead, we use the incremental error-based training procedure involving
a ``dynamic'' oracle proposed by  \citet{goldberg:12}, where the parser is trained to
predict the transition that will produce
the best-possible analysis from its current configuration.  We explained how to modify the Goldberg
and Nivre oracle so it predicts the optimal moves, either monotonic or non-monotonic,
from any configuration, and use this to train an averaged perceptron model.

When evaluated on the standard WSJ training and test sets we obtained a \uas of 91.1\%,
which is a 0.2\% improvement over the already state-of-the-art baseline of 90.9\% that we
obtain with the error-based training procedure restricted to monotonic transitions of
\citet{goldberg:12}.  We showed that this difference is significant using a Wilcoxon
test on the output of 20 runs of each system.

Looking to the future, we believe that it would be interesting to investigate whether
adding non-monotonic transitions is beneficial in other parsing systems as well, including
systems that target formalisms other than dependency grammars.  As we observed
in the paper, the spurious ambiguity that non-monotonic moves introduce may well be an
advantage in a statistical parser with an enormous state-space because it provides
multiple pathways to the correct analysis (of which we hope at least one is navigable).

We investigated a very simple kind of non-monotonic transition here, but of course it's
possible to design transition systems with many more transitions, including transitions
that are explicitly designed to ``repair'' characteristic parser errors.  It might even
be possible to automatically identify the most useful repair transitions and incorporate them
into the parser.

\bibliography{repair}
\bibliographystyle{aclnat}


\end{document}

