
\documentclass[11pt,letterpaper]{article}
\usepackage{acl2013}
\usepackage{times}
\usepackage{latexsym}
\usepackage{xspace}
\usepackage{natbib}
\usepackage{tikz-dependency}
\usepackage{placeins}
% Alter some LaTeX defaults for better treatment of figures:
    % See p.105 of "TeX Unbound" for suggested values.
    % See pp. 199-200 of Lamport's "LaTeX" book for details.
    %   General parameters, for ALL pages:
    \renewcommand{\topfraction}{0.9}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{0.8}	% max fraction of floats at bottom
    %   Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     % 2 may work better
    \setcounter{dbltopnumber}{2}    % for 2-column pages
    \renewcommand{\dbltopfraction}{0.9}	% fit big float above 2-col. text
    \renewcommand{\textfraction}{0.07}	% allow minimal text w. figs
    %   Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.7}	% require fuller float pages
	% N.B.: floatpagefraction MUST be less than topfraction !!
    \renewcommand{\dblfloatpagefraction}{0.7}	% require fuller float pages

	% remember to use [htp] or [htpb] for placement


\setlength\titlebox{6.5cm}    % Expanding the titlebox


\renewcommand{\tabcolsep}{5pt}

\newcommand{\baseacc}{00.00\xspace}
\newcommand{\sysacc}{00.00\xspace}
\newcommand{\sysimprove}{00.00\xspace}
\newcommand{\las}{\textsc{las}\xspace}
\newcommand{\uas}{\textsc{uas}\xspace}
\newcommand{\pp}{\textsc{pp}\xspace}
\newcommand{\pos}{\textsc{pos}\xspace}
\newcommand{\wsj}{\textsc{wsj}\xspace}

\newcommand{\stacktop}{S$_0$\xspace}
\newcommand{\buffone}{N$_0$\xspace}

\title{Removing Monotonicity Constraints Improves Accuracy for Arc-Eager Dependency Parsing}


\author{Author 1\\
	    XYZ Company\\
	    111 Anywhere Street\\
	    Mytown, NY 10000, USA\\
	    {\tt author1@xyz.org}
	  \And
	Author 2\\
  	ABC University\\
  	900 Main Street\\
  	Ourcity, PQ, Canada A1A 1T2\\
  {\tt author2@abc.ca}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
    Previous incremental parsers have assumed that state transitions
    should be monotonic. However, transitions can be made to revise
    previous decisions quite naturally, based on further information.
    Constraining this behaviour amounts
    to placing less trust in the model, which we can expect to be less productive as
    our models improve.

    We show how simple adjustments to the Arc-Eager transition system to relax the
    monotonicity constraint improves accuracy, especially when imperfect
    states are introduced during training. To support our claim that better models
    profit more from non-monotonic transitions, we present a set of
    additional features that improve accuracy substantially. Despite having fewer errors
    to correct, relaxing the monotonicity constraints is even more productive on
    the improved model, for a total improvement on the previous state-of-the-art of
    Y\%.


    %Previous algorithms for incremental parsing have assumed that state transititions
    %should be monotonic. However, psycholinguistic models of sentence processing
    %typically include moves that restructure the parse tree, in order to account
    %for eye-tracking and reading time experiments.

    %We present a non-monotonic transition system
    %that achieves state-of-the-art accuracy for greedy English dependency parsing.
    %We show that simply removing standard constraints on the Arc-Eager system
    %leads to useful non-monotonic behaviour, especially when imperfect states
    %are introduced during training. With the addition of a new transition
    %to correct attachment errors, the repair moves improve accuracy by 0.5\% on
    %the standard evaluation, over a state-of-the-art baseline.
\end{abstract}

% P1
\section{Introduction}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus dapibus. Phasellus elementum eleifend neque. Pellentesque non enim urna, in molestie nisi. Sed mattis orci a massa ultricies congue. Suspendisse euismod cursus dolor, a luctus elit ultrices et. Praesent at eros orci. Integer suscipit neque sit amet felis eleifend vel ullamcorper mauris congue. Sed imperdiet ultricies elit in adipiscing. Quisque condimentum consequat turpis at feugiat. Sed fringilla viverra quam, sed auctor nunc convallis vitae. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Phasellus vitae elit vel quam pulvinar cursus ut nec erat.

Maecenas sed sollicitudin nisl. Maecenas vehicula lacinia tristique. In nec sapien augue, eu tincidunt arcu. Nulla molestie lectus a sapien tincidunt a volutpat nisl consequat. Vivamus consectetur semper risus, nec pretium felis sagittis et. Nam tellus dolor, pulvinar non fermentum quis, auctor a metus. Proin mi purus, dignissim eget tempus in, vestibulum eu libero.

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed. Aliquam et augue est, et pretium est. Aenean mattis lacinia elit quis accumsan.

Nunc pellentesque magna et augue hendrerit dignissim. Donec tempus velit quis purus tincidunt placerat. Suspendisse venenatis aliquet elit id convallis. Praesent porttitor, libero eu pretium gravida, urna velit sollicitudin lectus, sit amet malesuada velit quam tristique diam.

% P2
% 2-col Dependency parse graphic
% Col 1
\begin{table*}[ht]
\centering
    \begin{tabular}{ll|l}
Transition & & Precondition \\
\hline \hline
Left-Arc   & (notation) & (S0 has no head) \\ 
Right-Arc  & (notation) &   \\
Reduce     & (notation) & (S0 has a head) \\  
Shift      & (notation) & \\
\end{tabular}
\caption{In the Arc-Eager transition system, an arc is created either when the word is
    pushed or popped from the stack. There are thus two pairs of operations: (Shift, Left)
    and (Right, Reduce). The choice of pop move is constrained by which push move
    was selected, so that the state is updated monotonically.\\
Instead, we give the model free choice of pop moves, and reverse the previous
decision if necessary. This allows the parser to recover from mistakes.}
\label{tab:transitions}
\end{table*}



\section{Transition-based Dependency Parsing}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus dapibus. Phasellus elementum eleifend neque. Pellentesque non enim urna, in molestie nisi. Sed mattis orci a massa ultricies congue. Suspendisse euismod cursus dolor, a luctus elit ultrices et. Praesent at eros orci. Integer suscipit neque sit amet felis eleifend vel ullamcorper mauris congue. Sed imperdiet ultricies elit in adipiscing. Quisque condimentum consequat turpis at feugiat. Sed fringilla viverra quam, sed auctor nunc convallis vitae. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Phasellus vitae elit vel quam pulvinar cursus ut nec erat.

Maecenas sed sollicitudin nisl. Maecenas vehicula lacinia tristique. In nec sapien augue, eu tincidunt arcu. Nulla molestie lectus a sapien tincidunt a volutpat nisl consequat. Vivamus consectetur semper risus, nec pretium felis sagittis et. Nam tellus dolor, pulvinar non fermentum quis.

\subsection{Dynamic oracle}

Maecenas sed sollicitudin nisl. Maecenas vehicula lacinia tristique. In nec sapien augue, eu tincidunt arcu. Nulla molestie lectus a sapien tincidunt a volutpat nisl consequat. Vivamus consectetur semper risus, nec pretium felis sagittis et. Nam tellus dolor, pulvinar non fermentum quis, auctor a metus. Proin mi purus, dignissim eget tempus in, vestibulum eu libero.


Maecenas sed sollicitudin nisl. Maecenas vehicula lacinia tristique. In nec sapien augue, eu tincidunt arcu. Nulla molestie lectus a sapien tincidunt a volutpat nisl consequat. Vivamus consectetur semper risus, nec pretium felis sagittis et. 

% Col 2
\subsection{Following non-gold transitions in training}
\begin{figure}
\centering
    \begin{verbatim}

while not state.is_finished {
  feats = extract(state)
  zero_costs = oracle(state)
  g = predict_from(feats, zero_costs)
  monotonic = check_constraints(state)
  p = predict_from(feats, monotonic)
  if p not in zero_costs {
     update(feats, g, 1)
     update(feats, p, -1)
  }
  transition(state, p)
}
\end{verbatim}
\caption{Dynamic oracle. TODO: Notation.}
\end{figure}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus dapibus. Phasellus elementum eleifend neque. Pellentesque non enim urna, in molestie nisi. Sed mattis orci a massa ultricies congue. Suspendisse euismod cursus dolor, a luctus elit ultrices et. Praesent at eros orci. Integer suscipit neque sit amet felis eleifend vel ullamcorper mauris congue. Sed imperdiet ultricies elit in adipiscing. Quisque condimentum consequat turpis at feugiat. Sed fringilla viverra quam, sed auctor nunc convallis vitae. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Phasellus vitae elit vel quam pulvinar cursus ut nec erat.

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus.

\begin{figure}
    \centering
    \begin{dependency}[theme=simple]
        \tikzstyle{t}=[text=green,ultra thick,font=\bfseries\itshape]
        \tikzstyle{f}=[text=red,ultra thick,font=\bfseries\itshape]
        \tikzstyle{m}=[font=\bfseries\itshape]
        \tikzstyle{n}=[font=\itshape]

        \begin{deptext}[column sep=.075cm, row sep=.1ex]
            I \&           \& saw \&          \& Jack       \& \& and     \&           \& Jill   \&  \& fall \& \textsc{r} \\
|[m]|S \& |[m]|L \& |[m]|S   \& |[f]|R \& |[m]|R \& |[m]|D \& |[m]|R \& |[m]|D \& |[t]|L \& |[n]|R \& |[n]|D \& |[n]|L \\
            1 \&     2       \& 3  \&   4      \& 5          \& 6 \& 7     \& 8 \& 9 \& 10 \& 11 \& 12 \\
    I \&           \& saw \&          \& Jack       \& \& and     \&           \& Jill   \&      \& fall \& \textsc{r} \\
        \end{deptext}
    \depedge{3}{1}{2}
    \depedge[red, ultra thick]{3}{5}{4}
    \depedge{5}{7}{5}
    \depedge{5}{9}{7}
    
    \deproot[edge height=0.7cm, ultra thick]{11}{}
    \wordgroup{1}{3}{3}{}
    \wordgroup{1}{5}{5}{}
    
    \depedge[edge below]{3}{1}{2}
    \depedge[edge below]{5}{7}{5}
    \depedge[edge below]{5}{9}{7}
    \depedge[edge below, green, ultra thick]{11}{5}{9}
    \wordgroup{4}{3}{3}{}
\end{dependency}
\caption{
    \small
    State before and after a non-monotonic Left-Arc.
    At move 9, \emph{fall} is the first word of the buffer (marked with an arrow),
    and \emph{saw} and \emph{Jack} are on the stack (circled). The arc created at move 4 was
    incorrect (in red). Arcs are labelled with the move that created them.
    After move 9 (the lower state), the non-monotonic Left-Arc move
    has replaced the incorrect dependency with a correct Left-Arc (in green).
\label{fig:clobber}}
\end{figure}



\section{The Non-Monotonic Arc-Eager Transition System}

The Arc-Eager transition system \citep{nivre:04} has four moves. Two create 
dependencies, two push the first word of the buffer to the stack, and two pop 
the stack:

\begin{center}
    \begin{tabular}{l|cc}
             & Push  & Pop    \\
           \hline
           Sets head   & \emph{Right} & \textbf{Left}    \\
            No head    & \textbf{Shift} & \emph{Reduce}   \\
     \end{tabular}
\end{center}

Every word in the sentence is pushed once and popped once; and every
word must have exactly one head. This creates two pairings, along the
diagonals: (S, L) and (R, D). Every word must acquire
its head when it is pushed or when it is popped, but not both, and not neither.

The Arc-Eager system guarantees this by constraining the choice of pop move:

\begin{itemize}\setlength{\itemsep}{-2mm}
    \item Iff $S_0$ has a head, Reduce;
    \item Iff $S_0$ has no head, Left-Arc.
\end{itemize}
In the existing solution, the pair is decided by the first move.
Our idea is to let the pair be decided by the \emph{second} move.

This makes the transition system \emph{non-monotonic}, because if the model decides
on an incongruent pairing
we will have to either undo or add a
dependency, depending on whether we correct a Right-Arc with a Left-Arc, or correct
a Shift with a Reduce. We now describe these operations.

\subsection{Non-monotonic Left-Arc}

The upper arcs in Figure \ref{fig:clobber} show a transition history where the wrong
push move was selected. The parser began correctly by Shifting \emph{I}
and Left-Arcing it to \emph{saw}, which was then also Shifted. The incorrect move,
4 (in red), is to Right-Arc \emph{Jack} instead of Shifting it.

The difficulty of this kind of sentence for an incremental parser is fundamental.
The leftward context does not constrain the decision, 
and an arbitrary amount of text could separate \emph{Jack} from its true head,
\emph{fall}. Eye-tracking
experiments show that humans often perform a saccade while reading such examples (TODO).

In moves 5-8 the parser correctly builds the rest of the \textsc{np}, and arrives
at \emph{fall}. The monotonicity constraints would here force an incorrect analysis,
even though the evidence for the correct decision is much stronger at the later move.

We allow Left-Arcs to `clobber' Right-Arcs if the model
recommends it. The Right-Arc is deleted, and the Left-Arc proceeds as normal. The
effect of this is exactly as if the model had correctly chosen Shift at
move 4. We simply give the model a second chance to make the correct choice.

\begin{figure}
    \centering
    \begin{dependency}[theme=simple]
    \tikzstyle{t}=[text=green,ultra thick,font=\bfseries\itshape]
    \tikzstyle{f}=[text=red,ultra thick,font=\bfseries\itshape]
    \tikzstyle{m}=[font=\bfseries\itshape]
    \tikzstyle{n}=[font=\itshape]
    \begin{deptext}[column sep=.075cm, row sep=.1ex]
        I \&           \& saw \&          \& Jack       \& \& and     \&           \& Jill \&   \& \textsc{r} \\
       |[m]|S \& |[m]|L \& |[m]|S   \& |[f]|S \& |[m]|R \& |[m]|D \& |[m]|R \& |[m]|D \& |[m]|R \& |[m]|D \& |[t]|D \& |[n]|L \\
            1 \&     2       \& 3  \&   4      \& 5          \& 6 \& 7     \& 8 \& 9 \& 10 \& 11 \& 12 \\
            I \&           \& saw \&          \& Jack       \& \& and     \&           \& Jill \& \& \textsc{r} \\
\end{deptext}
    \depedge{3}{1}{2}
    \depedge{5}{7}{5}
    \depedge{5}{9}{7}
    
    \deproot[edge height=0.7cm, ultra thick]{11}{}
    \wordgroup{1}{3}{3}{}
    \wordgroup{1}{5}{5}{}
    
    \depedge[edge below]{3}{1}{2}
    \depedge[edge below]{5}{7}{5}
    \depedge[edge below]{5}{9}{7}
    \depedge[edge below, green, ultra thick]{3}{5}{9}
    \wordgroup{4}{3}{3}{}
\end{dependency}
\caption{
\small
State before and after a non-monotonic Reduce.
After making the wrong push move at 4, at move 11
the parser has \emph{Jack} on the stack (circled), with only the dummy \textsc{root}
token left in the buffer. A monotonic parser must deterministically Left-Arc
\emph{Jack} here to preserve the previous decision, despite the current state.
We remove this constraint, and instead assume that when the model selects Reduce
for a headless item, it is reversing the previous Shift/Right decision. We add
the appropriate arc, assigning the label that scored highest when the Shift/Right
decision was originally made.
\label{fig:adduce}}
\end{figure}

\subsection{Non-monotonic Reduce}

The upper arcs in Figure \ref{fig:adduce} show a state resulting from the opposite error.
The parser has Shifted \emph{Jack} instead of Right-Arcing it. After
building the \textsc{np} the buffer is exhausted, except for the artificial root token,
which we use to Left-Arc the head of the sentence, following \citet{nivre:squib}.

Instead of letting the previous choice lock us in to the pair (Shift, Left), we let
the latter decision reverse it to (Right, Reduce). This gives the model the flexibility
to take advantage of the extra information, and make what should be an easy decision.
When the Shift/Right decision is reversed, we add an arc between the top of the stack
and the word above it. This is the arc that would have been created had the parser
chosen to Right-Arc when it chose to Shift. Since our idea is to reverse this mistake,
we select the Right-Arc label that the model scored most highly at that time.


\subsection{Why have two push moves?}

We have argued above that it is better to trust the second decision that the model
makes, rather than using the first decision to determine the second. If this is
the case, is the first decision entirely redundant?
Instead of defining how pop moves can correct Shift/Right mistakes, we could
instead eliminate the ambiguity. There are two possibilities:
Shift every token, and create all Right-Arcs via Reduce; or Right-Arc every token,
and override the incorrect arcs with subsequent Left-Arcs.

The problem with these approaches is that it is actually very useful to condition
on the results of previous decisions. Saving all of the difficulty for later
is not a very good structured prediction strategy. Most of the transition decisions are very
easy, and when the parser does encounter a difficult decision, it is advantageous
to have the dependency structure as a summary of the previous information, in
much the same way that the previous tags are critical for \pos tagging. 

As an example of the problem, if the Shift move is
eliminated, about half of the Right-Arcs created will be spurious. All of these
arcs will be assigned labels\footnote{If the parser is trained to label the temporary
arcs differently, its action is roughly (but not fully) isomorphic to the Shift move.},
making important features uselessly noisy.

The non-monotonic transition system that we propose does not have this problem.
The model makes Shift vs. Right decisions as normal, and conditions on them --- but
without \emph{committing} to them.

\subsection{Defining the oracle}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus dapibus. Phasellus elementum eleifend neque. Pellentesque non enim urna, in molestie nisi. Sed mattis orci a massa ultricies congue. Suspendisse euismod cursus dolor, a luctus elit ultrices et. Praesent at eros orci. Integer suscipit neque sit amet felis eleifend vel ullamcorper mauris congue.

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus dapibus. Phasellus elementum eleifend neque. Pellentesque non enim urna, in molestie nisi. Sed mattis orci a massa ultricies congue. Suspendisse euismod cursus dolor, a luctus elit ultrices et. Praesent at eros orci. Integer suscipit neque sit amet felis eleifend vel ullamcorper mauris congue.

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus dapibus. Phasellus elementum eleifend neque. Pellentesque non enim urna, in molestie nisi. Sed mattis orci a massa ultricies congue. Suspendisse euismod cursus dolor, a luctus elit ultrices et. Praesent at eros orci. Integer suscipit neque sit amet felis eleifend vel ullamcorper mauris congue.


Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus dapibus. Phasellus elementum eleifend neque. Pellentesque non enim urna, in molestie nisi. Sed mattis orci a massa ultricies congue. Suspendisse euismod cursus dolor, a luctus elit ultrices et. Praesent at eros orci. Integer suscipit neque sit amet felis eleifend vel ullamcorper mauris congue.

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus dapibus. Phasellus elementum eleifend neque. Pellentesque non enim urna, in molestie nisi. Sed mattis orci a massa ultricies congue. Suspendisse euismod cursus dolor, a luctus elit ultrices et.

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus dapibus. Phasellus elementum eleifend neque. Pellentesque non enim urna, in molestie nisi. Sed mattis orci a massa ultricies congue. Suspendisse euismod cursus dolor, a luctus elit ultrices et. Praesent at eros orci. Integer suscipit neque sit amet felis eleifend vel ullamcorper mauris congue.

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus dapibus. Phasellus elementum eleifend neque. Pellentesque non enim urna, in molestie nisi. Sed mattis orci a massa ultricies congue. Suspendisse euismod cursus dolor, a luctus elit ultrices et. Praesent at eros orci. Integer suscipit neque sit amet felis eleifend vel ullamcorper mauris congue.
% P5
\section{Experiments}

We implemented a standard Arc-Eager parser trained with the averaged Perceptron,
and used it to evaluate the non-monotonic transitions. We tested the transitions on
multiple dependency schemes, to check whether any effects were specific to annotation
contingencies. We also analyse the effect on accuracy by dependency type, and
investigate the impact on error propagation.


\begin{table}[t]
    \small
    \centering
    \begin{tabular}{l|rrrr}
        \hline
                              & Stanford & MALT & CoNLL & ?? \\
        \hline \hline
                              & \multicolumn{4}{c}{Unlabelled Attachment Scores} \\
        \hline
        Baseline              &  00.0  & 00.0 & 00.0 & 00.0   \\
        NM L                  &  00.0  & 00.0 & 00.0 & 00.0   \\
        NM D                  &  00.0  & 00.0 & 00.0 & 00.0   \\
        NM L+D                &  00.0  & 00.0 & 00.0 & 00.0    \\ 
        \hline
                              & \multicolumn{4}{c}{Labelled Attachment Score} \\
        \hline
        Baseline              &  00.0  & 00.0 & 00.0  & 00.0   \\
        NM L                  &  00.0  & 00.0 & 00.0  & 00.0   \\
        NM D                  &  00.0  & 00.0 & 00.0  & 00.0   \\
        NM L+D                &  00.0  & 00.0 & 00.0  & 00.0   \\ 
        \hline
    \end{tabular}
    \caption{\small Effect of non-monotonic transitions with \textbf{standard training}.
             All results averaged across 20 runs with different random seeds. All models
             had $\sigma$=TODO-TODO.\label{tab:standard}}
\end{table}


\subsection{Data and Evaluation}

Nunc pellentesque magna et augue hendrerit dignissim. Donec tempus velit quis purus tincidunt placerat. Suspendisse venenatis aliquet elit id convallis. Praesent porttitor, libero eu pretium gravida, urna velit sollicitudin lectus, sit amet malesuada velit quam tristique diam. Nunc sem mauris, volutpat nec rhoncus quis, sodales ut massa. Praesent egestas ante interdum elit laoreet volutpat. Morbi quis velit tempor libero consequat volutpat at vel nulla. Morbi et justo ac liberos.
% P6

\subsection{Parsing model}

We base our experiments on the parser described by \citet{goldberg:12}. We
began by implementing their baseline system, a standard Arc-Eager parser using
an averaged Perceptron learner and the extended feature set described by \citet{zhang:11}.
We follow \citeauthor{goldberg:12} in training all models for 15 iterations,
and in shuffling the sentences before each iteration. This randomisation is
important for accuracy, but causes variance, so accuracies
are reported as the average of 20 runs. All experiments had a standard deviation between
TODO and TODO\%.

Labels and moves were learned jointly, resulting in a single model with 88 classes
for the Stanford dependencies. Joint label/move learning proved surprisingly
advantageous, giving 0.4\% additional \uas.

We follow the suggestion of \citet{nivre:squib} to handle root-node dependencies by
adding a dummy token at the \emph{end} of the sentence.
This improves accuracy by delaying root-node decisions, and likely accounts for the
0.2\% extra accuracy of our standard-trained baseline system over the equivalent result
reported by \citet{goldberg:12}. The only other known difference between the systems
is a modest model compaction step performed by the proprietary Google perceptron
code.

\subsection{Training strategies}

\textbf{Standard.} Nunc pellentesque magna et augue hendrerit dignissim. Donec tempus velit quis purus tincidunt placerat. Suspendisse venenatis aliquet elit id convallis. Praesent porttitor, libero eu pretium gravida, urna velit sollicitudin lectus, sit amet malesuada velit quam tristique diam. Nunc sem mauris, volutpat nec rhoncus quis,

\textbf{Goldberg and Nivre.} Nunc pellentesque magna et augue hendrerit dignissim. Donec tempus velit quis purus tincidunt placerat. Suspendisse venenatis aliquet elit id convallis. Praesent porttitor, libero eu pretium gravida.


\begin{table}[t]
    \small
    \centering
    \begin{tabular}{l|rrrr}
        \hline
                              & Stanford & MALT & CoNLL & ?? \\
        \hline \hline
                              & \multicolumn{4}{c}{Unlabelled Attachment Scores} \\
        \hline
        Baseline              &  00.0  & 00.0 & 00.0 & 00.0   \\
        NM L                  &  00.0  & 00.0 & 00.0 & 00.0   \\
        NM D                  &  00.0  & 00.0 & 00.0 & 00.0   \\
        NM L+D                &  00.0  & 00.0 & 00.0 & 00.0    \\ 
        \hline
                              & \multicolumn{4}{c}{Labelled Attachment Score} \\
        \hline
        Baseline              &  00.0  & 00.0 & 00.0  & 00.0   \\
        NM L                  &  00.0  & 00.0 & 00.0  & 00.0   \\
        NM D                  &  00.0  & 00.0 & 00.0  & 00.0   \\
        NM L+D                &  00.0  & 00.0 & 00.0  & 00.0   \\ 
        \hline
    \end{tabular}
    \caption{\small Effect of non-monotonic transitions with \textbf{Goldberg and Nivre training}
            All results averaged across 20 runs with different random seeds. All models
             had $\sigma$=0.07-0.08.\label{tab:goldberg}}
\end{table}

\subsection{Transition systems}

\textbf{Baseline.} Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus dapibus. Phasellus elementum eleifend neque. Pellentesque non enim urna, in molestie nisi. Sed mattis orci a massa ultricies congue. Suspendisse euismod cursus dolor.

\textbf{NM L.} Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue.

\textbf{NM D.} Non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor.

\textbf{NM L+D.} Non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.


\section{Development Results}
\label{sec:results}

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.


\begin{table}
\centering
\small
\begin{tabular}{lrrrr}
        & Freq. & Base & NM & WD \\
    \hline \hline
    \multicolumn{5}{c}{Left-Arcs} \\
    \hline
    det	& 3350 & 00.0 & +0.0 & 0.0 \\
     nn	& 3208 & 00.0 &	-0.0 & 0.0 \\
  nsubj	& 2691 & 00.0 &	0.0 & 0.0 \\
   amod	& 2428 & 00.0 & 0.0 & 0.0 \\
aux	    & 1229 & 00.0 &	0.0 & 0.0 \\
advmod  & 810  & 00.0 & 0.0 & 0.0 \\
num	    & 749  & 00.0 & 0.0 & 0.0 \\
   poss	& 707  & 00.0 & 0.0 & 0.0 \\
  Other	& 2637 & 00.0 & 0.0 & 0.0 \\
\hline
\multicolumn{5}{c}{Right-Arcs} \\
\hline
pobj	& 3739  & 00.0 & 0.0 & 0.0 \\
prep	& 3498  & 00.0 & 0.0 & 0.0 \\
dobj	& 1568  & 00.0 & 0.0 & 0.0 \\
conj	& 1002  & 00.0 & 0.0 & 0.0 \\
cc	    & 898   & 00.0 & 0.0 & 0.0 \\
number	& 468   & 00.0 & 0.0 & 0.0 \\
 ccomp	& 449   & 00.0 & 0.0 & 0.0 \\
 xcomp	& 438   & 00.0 & 0.0 & 0.0 \\
advmod	& 437   & 00.0 & 0.0 & 0.0 \\
ps	    & 425   & 00.0 & 0.0 & 0.0 \\
dep	    & 406   & 00.0 & 0.0 & 0.0 \\
  Other	& 2549  & 00.0 & 0.0 & 0.0 \\
\hline
All left  & 000 & 00.0 & 0.0 & 0.0  \\
All right & 000 & 00.0 & 0.0 & 0.0  \\
Root    & 1700 & 00.0 & 0.0 & 0.0    \\     
\hline
\end{tabular}
\caption{\small Accuracies for common Stanford labels,
         using G\&N training and standard features.
         \textbf{NM L+D} accuracy is shown by difference from the baseline. The
         WD column shows the difference weighted by the frequency of the label.
     \label{tab:labels}}
\end{table}


\subsection{Accuracy by label}

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.


\subsection{Effect on error propagation}

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.

\begin{table}
    \small
    \centering
    \begin{tabular}{l|rrr|rrr}
        & \multicolumn{3}{c|}{Standard Training} & \multicolumn{3}{c}{G\&N Training}\\
        $E$   & N & Baseline & NM    & N &Baseline & NM    \\
        \hline \hline
        0     &   &          &        &   &       &        \\
        1     &   &          &        &   &       &        \\
        2     &   &          &        &   &       &        \\
        3     &   &          &        &   &       &        \\
        4     &   &          &        &   &       &        \\
        5     &   &          &        &   &       &        \\
        $>5$  &   &          &        &   &       &        \\
        \hline
    \end{tabular}
\caption{\small Likelihood of making a transition error in a state given $E$ previous
         errors with the two training strategies. The system accuracy (NM L+D) is
     shown as a difference from the baseline.\label{tab:errprop}}
\end{table}

\subsection{Interaction with additional features}

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.
\begin{table}
\small

\centering
(S1w), (S1p), (S1w, S1p),\\
(S1w, N0w), (S1w, N0p), (S1p, N0p),\\
(S1w, S0w, N0w), (S1p, S0p, N0p), (S1p, S0p, N0p),\\
(S2w, N0w),\\
(S2w, N1w),\\
(S2p, N0p, N1w), (S2p, N0w, N1w), (S2w, N0p, N1p), \\
(S3w, N0w), \\
(S2w, N1w),\\ 
(S3p, S2p, N0w),\\
(dist, S1w, N1w),\\
(dist, S1p, N0p, N1p)
\caption{Additional features. S=Stack, N=buffer. w=Word, p=\pos. Dist=Distance
     between S0 and N0. TODO: Prettify this. TODO: Update it, it's from the old version.}
\end{table}

\begin{table}
\centering
\begin{tabular}{lrr}
    \hline 
            & \las  & \uas  \\
    \hline \hline 
                & \multicolumn{2}{c}{Standard Features} \\
                \hline
    Baseline    & 00.0 & 00.0 \\
    NM L+D      & 00.0 & 00.0 \\
\hline
                & \multicolumn{2}{c}{Additional features} \\
                \hline
    Baseline    & 00.0 & 00.0 \\
    NM L+D      & 00.0 & 00.0 \\
\end{tabular}
\caption{
    \small
    Impact of the additional features on Stanford \wsj22 using the baseline and
    non-monotonic transition systems.
\label{tab:feats}}
\end{table}


\section{Test Results}

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.


\begin{table}
    \centering
    \small
    \begin{tabular}{l|r|rr|rr}
        \hline 
System  &   $O$      & \multicolumn{2}{c|}{Penn2Malt} & \multicolumn{2}{c}{Stanford} \\
                           &          & \las  & \uas  & \las & \uas \\
        \hline \hline
K\&C 10  & $n^3$ & ---   & 93.00 & ---  & --- \\
Z\&N 11  & $nk$  & 91.8  & 92.9  & 91.9 & 93.5\\
G\&N 12  & $n$   & ---   & ---   & 88.72 & 90.96 \\
        \hline
Baseline    & $n$ &       &       &       &  \\
NM L+D & $n$ &       &       &       &  \\
\hline
Base + Feats   & $n$ &       &       &       &  \\
NM L+D + Feats & $n$ &       &       &       &  \\
\hline
    \end{tabular}
    \caption{Test results on \wsj 23. 
         \label{tab:feats}}
\end{table}



% P8
\section{Related Work}


Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.

\subsection{Parse restructuring in psycholinguistics}

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.

\section{Conclusion}

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.

\bibliography{repair}
\bibliographystyle{aclnat}


\end{document}
