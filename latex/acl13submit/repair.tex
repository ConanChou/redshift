
\documentclass[11pt,letterpaper]{article}
\usepackage{acl2013}
\usepackage{times}
\usepackage{latexsym}
\usepackage{xspace}
\usepackage{natbib}
\usepackage{tikz-dependency}


\setlength\titlebox{6.5cm}    % Expanding the titlebox

\newcommand{\baseacc}{00.00\xspace}
\newcommand{\sysacc}{00.00\xspace}
\newcommand{\sysimprove}{00.00\xspace}
\newcommand{\las}{\textsc{las}\xspace}
\newcommand{\uas}{\textsc{uas}\xspace}
\newcommand{\pp}{\textsc{pp}\xspace}
\newcommand{\pos}{\textsc{pos}\xspace}
\newcommand{\wsj}{\textsc{wsj}\xspace}

\newcommand{\stacktop}{S$_0$\xspace}
\newcommand{\buffone}{N$_0$\xspace}

\title{Improving Incremental Dependency Parsing with Repair Transitions}


\author{Author 1\\
	    XYZ Company\\
	    111 Anywhere Street\\
	    Mytown, NY 10000, USA\\
	    {\tt author1@xyz.org}
	  \And
	Author 2\\
  	ABC University\\
  	900 Main Street\\
  	Ourcity, PQ, Canada A1A 1T2\\
  {\tt author2@abc.ca}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
    Previous approaches to incremental parsing have assumed that the parser
    should build its analysis monotonically. We show that relaxing this constraint
    can offer an improved transition system for English dependency parsing.

    We modify the Left-Arc of the Arc-Eager transition system so that it can 
    over-ride a previous Right-Arc. We then remove the Shift move, as there is no
    reason not to create a provisional dependency when a word is pushed onto the stack.

    We train the parser using the online learning algorithm described by
    \citet{goldberg:12}, which allows training from states resulting from
    transition errors. The updated transition system improves \uas by
    0.5\%, to \sysacc, setting a new state-of-the-art for greedy dependency parsing.
\end{abstract}

% P1
\section{Introduction}

\clearpage

% P2
% 2-col Dependency parse graphic
% Col 1
\section{Transition-based Dependency Parsing}
\pagebreak
% Col 2
\subsection{Dynamic Oracle}
\begin{figure}
\begin{dependency}[theme=simple]

\tikzstyle{t}=[text=green,ultra thick,font=\bfseries\itshape]
\tikzstyle{f}=[text=red,ultra thick,font=\bfseries\itshape]
\tikzstyle{m}=[font=\bfseries\itshape]

\begin{deptext}[column sep=.1cm, row sep=.1ex]
 The \&   \& man    \&      \& whisting \&    \& tunes \&    \& pianos \& \\
|[m]|S \& |[m]|L \& |[m]|S \& |[m]|R \& |[f]|R \& \\
    1   \& 2 \& 3      \& 4    \& 5         \& 6 \&   7    \& 8 \&    9    \& 10 \\
\end{deptext}

\depedge{3}{1}{2}
\depedge{3}{5}{4}
\depedge[red, ultra thick]{5}{7}{5}
\deproot[edge height=0.7cm, ultra thick]{9}{}
\wordgroup{1}{3}{3}{}
\wordgroup{1}{5}{5}{}
\wordgroup{1}{7}{7}{}
\end{dependency}
\caption{
An example of the error propagation problem.
The correct attachment of \emph{pianos} to \emph{tunes} may be
prevented by the incorrect attachment of \emph{tunes} to \emph{whistling},
especially if the parser has not been trained on suboptimal states.
Words on the stack are circled and the transition-history is shown, with arcs
labelled by the move that introduced them. The first word of the buffer is marked
with an arrow. Incorrect transitions and arcs are highlghted in red.
\label{fig:whistling_tunes}}
\end{figure}

\pagebreak


% P3&4
\section{The Arc-Always Transition System}
\begin{figure}
    \centering
    \begin{dependency}[theme=simple]
        \tikzstyle{t}=[text=green,ultra thick,font=\bfseries\itshape]
        \tikzstyle{f}=[text=red,ultra thick,font=\bfseries\itshape]
        \tikzstyle{m}=[font=\bfseries\itshape]

        \begin{deptext}[column sep=.1cm, row sep=.1ex]
    I \&           \& saw \&          \& Jack       \& \& and     \&           \& Jill   \&      \& fall \\
|[m]|S \& |[m]|L \& |[m]|S   \& |[m]|S \& |[m]|L \& |[f]|R \& |[m]|R \& |[m]|D \& |[t]|L    \\
            1 \&     2       \& 3  \&   4      \& 5          \& 6 \& 7     \& 8 \& 9 \& 10 \& 11 \& 12 \\
    I \&           \& saw \&          \& Jack       \& \& and     \&           \& Jill   \&      \& fall \\
        \end{deptext}
    \depedge{3}{1}{2}
    \depedge{7}{5}{5}
    \depedge[red, ultra thick]{3}{7}{6}
    \depedge{7}{9}{7}
    \deproot[edge height=0.7cm, ultra thick]{11}{}
    \wordgroup{1}{3}{3}{}
    \wordgroup{1}{7}{7}{}

    \depedge[edge below]{3}{1}{2}
    \depedge[edge below]{7}{5}{6}
    \depedge[edge below]{7}{9}{5}
    \depedge[edge below, green, ultra thick]{11}{7}{9}
    \wordgroup{4}{3}{3}{}
\end{dependency}
\caption{Before and after for the modified Left-arc transition.
    Left-arc (L) is modified so that it can over-ride, a previous
    right-arc pointing to \stacktop.
\label{fig:saw_fall}}
\end{figure}
\subsection{Oracle}

\clearpage


% P5
\section{Experiments}

\subsection{Parsing Model}

In order to test repair transitions, we first implemented a baseline greedy
deterministic parser.
We followed \citet{zhang:11} in using arc-eager transitions, with joint
prediction of arc labels for the left and right transitions.
However, we use a logistic
regression classifier, where \citet{zhang:11} and \citet{goldberg:12} use globally
normalised averaged perceptron models. We use the logistic regression
implementation in the
LibLinear toolkit\footnote{http://www.csie.ntu.edu.tw/\~cjlin/liblinear/}.
Features that occurred 5 or fewer times
were pruned before training, and L1-regularisation with a penalty of 1.0 was
used during training. Experiments on the development data showed that accuracies
were not significantly higher when the regularisation penalty was tuned
specifically for each model.

We use the extended feature set from \citet{zhang:11} in our experiments, which
is also the feature set used by \citet{goldberg:12}. 


%However, this feature set
%lacked features we expected would be needed to learn our repair moves, so we
%experimented with additional features, and found that they substantially improved
%the performance of the baseline model as well.

%After some analysis, we determined that a greedy parser greatly benefits from
%features that may not be required for beam-search parsing. 
%Figure \ref{fig:pp_attach} shows the state of a parser before
%it makes a \pp attachment decision, a common source parser error. Although
%feature engineering was the
%main contribution of \citet{zhang:11}, they lack a feature that pairs the verb
%\emph{put} with the preposition \emph{on} at this state. Instead, their parser will
%score the incorrect transition highly, but subsequent states along that path should
%later score poorly and be pruned from the beam.

%Table \ref{tab:features} shows our additional features. 
%\begin{table}
%    \small
    
%    \centering
%    (S1w), (S1p), (S1w, S1p),\\
%    (S1w, N0w), (S1w, N0p), (S1p, N0p),\\
%    (S1w, S0w, N0w), (S1p, S0p, N0p), (S1p, S0p, N0p),\\
%    (S2w, N0w),\\
%    (S2w, N1w),\\
%    (S2p, N0p, N1w), (S2p, N0w, N1w), (S2w, N0p, N1p), \\
%    (S3w, N0w), \\
%    (S2w, N1w),\\ 
%    (S3p, S2p, N0w),\\
%    (dist, S1w, N1w),\\
%    (dist, S1p, N0p, N1p)
%\caption{Additional features. S=Stack, N=buffer. w=Word, p=\pos. Dist=Distance
%         between S0 and N0. TODO: Prettify this.}
%\end{table}

%\begin{table}
%    \centering
%    \begin{tabular}{l|rr}
%        \hline 
%        Feature Set & \las  & \uas  \\
%        \hline \hline
%        Z\&N 11  & 88.20 & 89.54 \\
%+Additional      & 89.10 & 90.40 \\
%    \hline
%    \end{tabular}
%    \caption{Development set accuracies for our parser trained
%             with the original oracle, which is restricted
%             to states along the path to the gold derivation.
%             Our additional features improve accuracy, while label
%         features hinder performance due to error propagation.\label{tab:feats}}
%\end{table}

%\subsection{Iterative Training}

\subsection{Data and Evaluation}
\clearpage
% P6

% P7
\subsection{Results}
\label{sec:results}

\begin{table}
    \small
    \centering
    \begin{tabular}{l|rr}
        \hline 
        System             & \uas   & \las  \\
        \hline \hline
        Static Arc-Eager   & 00.00  & 00.00 \\
        Static Arc-Always  & 00.00  & 00.00 \\
        \hline \hline
        Dynamic Arc-Eager  & 00.00  & 00.00 \\
        Dynamic Arc-Always & 00.00  & 00.00 \\
        +Right-Lower       & 00.00  & 00.00 \\
        +Right-Raise       & 00.00  & 00.00 \\
        +Left-Invert       & 00.00  & 00.00 \\
        \hline
    \end{tabular}
    \caption{Dev set results.\label{tab:feats}}
\end{table}


\begin{table}
    \centering
    \small
    \begin{tabular}{l|r|rr|rr}
        \hline 
System  &          & \multicolumn{2}{c|}{Penn2Malt} & \multicolumn{2}{c}{Stanford} \\
                           &          & \las  & \uas  & \las & \uas \\
        \hline \hline
K\&C 10  & $O(n^3)$ & ---   & 93.00 & ---  & --- \\
Z\&N 11  & $O(nk)$  & 91.8  & 92.9  & 91.9 & 93.5\\
G\&N 12  & $O(n)$   & ---   & ---   & 88.72 & 90.96 \\
        \hline
Arc-Eager  & $O(n)$ &       &       &       &  \\
Arc-Always & $O(n)$ &       &       &       &  \\
    \end{tabular}
    \caption{Test set results
         \label{tab:feats}}
\end{table}

\clearpage


\section{Why is Arc-Always better?}

\subsection{Effect of Left-Reattach on Arc-Eager parsing}

\pagebreak
Describe the \emph{repair only} configuration and give its results


\subsection{Effect of unconstrained Reduce on Arc-Eager parsing}

Show what happens if we just remove the reduce constraint that says a word has
to have a head specified (since we always have a head, maybe this is the relevant
difference.)


\pagebreak

\subsection{Label accuracy analysis}

\begin{table}
\centering
\small
\begin{tabular}{lr|rr|rr}
    Label        & n & Arc-Eager & Arc-Always & Diff & Weighted diff \\
    \hline \hline
    Left labels  &          &           &            &      &               \\
    \hline
    Right labels &          &          &            &      &               \\
\end{tabular}
\end{table}

\subsection{Error propagation analysis}

If you're in a state and you've previously made $n$ errors, what's your probability
of making an error? Plot for static and online arc-eager and arc-always parsers.

\clearpage

% P8
\section{Related Work}
\pagebreak

\section{Conclusion}


\bibliography{repair}
\bibliographystyle{aclnat}


\end{document}
