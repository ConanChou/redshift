% vim: set textwidth=78 fo+=t :

\documentclass[11pt,letterpaper]{article}
\usepackage{acl2013}
\usepackage{amsmath, amsthm}
\usepackage{times}
\usepackage{latexsym}
\usepackage{xspace}
\usepackage{natbib}
\usepackage{tikz-dependency}
\usepackage{placeins}
\usepackage{xcolor}
% Alter some LaTeX defaults for better treatment of figures:
    % See p.105 of "TeX Unbound" for suggested values.
    % See pp. 199-200 of Lamport's "LaTeX" book for details.
    %   General parameters, for ALL pages:
    \renewcommand{\topfraction}{0.9}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{0.8}	% max fraction of floats at bottom
    %   Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     % 2 may work better
    \setcounter{dbltopnumber}{2}    % for 2-column pages
    \renewcommand{\dbltopfraction}{0.9}	% fit big float above 2-col. text
    \renewcommand{\textfraction}{0.07}	% allow minimal text w. figs
    %   Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.7}	% require fuller float pages
	% N.B.: floatpagefraction MUST be less than topfraction !!
    \renewcommand{\dblfloatpagefraction}{0.7}	% require fuller float pages

	% remember to use [htp] or [htpb] for placement


\setlength\titlebox{6.5cm}    % Expanding the titlebox


\renewcommand{\tabcolsep}{5pt}

\newcommand{\baseacc}{00.00\xspace}
\newcommand{\sysacc}{00.00\xspace}
\newcommand{\sysimprove}{00.00\xspace}
\newcommand{\las}{\textsc{las}\xspace}
\newcommand{\uas}{\textsc{uas}\xspace}
\newcommand{\pp}{\textsc{pp}\xspace}
\newcommand{\pos}{\textsc{pos}\xspace}
\newcommand{\wsj}{\textsc{wsj}\xspace}

\newcommand{\stacktop}{S$_0$\xspace}
\newcommand{\buffone}{N$_0$\xspace}

\newcommand{\tuple}[1]{$\langle#1\rangle$}
\newcommand{\maybe}[1]{\textcolor{gray}{#1}}

\title{Removing Monotonicity Constraints Improves Accuracy for Arc-Eager Dependency Parsing}


\author{Author 1\\
	    XYZ Company\\
	    111 Anywhere Street\\
	    Mytown, NY 10000, USA\\
	    {\tt author1@xyz.org}
	  \And
	Author 2\\
  	ABC University\\
  	900 Main Street\\
  	Ourcity, PQ, Canada A1A 1T2\\
  {\tt author2@abc.ca}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
    Previous incremental parsers have assumed that state transitions
    should be monotonic. However, transitions can be made to revise
    previous decisions quite naturally, based on further information.
    Constraining this behaviour amounts
    to placing less trust in the model, which we can expect to be less productive as
    our models improve.

    We show how simple adjustments to the Arc-Eager transition system to relax the
    monotonicity constraint improves accuracy, especially when imperfect
    states are introduced during training. To support our claim that better models
    profit more from non-monotonic transitions, we present a set of
    additional features that improve accuracy substantially. Despite having fewer errors
    to correct, relaxing the monotonicity constraints is even more productive on
    the improved model, for a total improvement on the previous state-of-the-art of
    Y\%.


    %Previous algorithms for incremental parsing have assumed that state transititions
    %should be monotonic. However, psycholinguistic models of sentence processing
    %typically include moves that restructure the parse tree, in order to account
    %for eye-tracking and reading time experiments.

    %We present a non-monotonic transition system
    %that achieves state-of-the-art accuracy for greedy English dependency parsing.
    %We show that simply removing standard constraints on the Arc-Eager system
    %leads to useful non-monotonic behaviour, especially when imperfect states
    %are introduced during training. With the addition of a new transition
    %to correct attachment errors, the repair moves improve accuracy by 0.5\% on
    %the standard evaluation, over a state-of-the-art baseline.
\end{abstract}

% P1
\section{Introduction}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus dapibus. Phasellus elementum eleifend neque. Pellentesque non enim urna, in molestie nisi. Sed mattis orci a massa ultricies congue. Suspendisse euismod cursus dolor, a luctus elit ultrices et. Praesent at eros orci. Integer suscipit neque sit amet felis eleifend vel ullamcorper mauris congue. Sed imperdiet ultricies elit in adipiscing. Quisque condimentum consequat turpis at feugiat. Sed fringilla viverra quam, sed auctor nunc convallis vitae. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Phasellus vitae elit vel quam pulvinar cursus ut nec erat.

Maecenas sed sollicitudin nisl. Maecenas vehicula lacinia tristique. In nec sapien augue, eu tincidunt arcu. Nulla molestie lectus a sapien tincidunt a volutpat nisl consequat. Vivamus consectetur semper risus, nec pretium felis sagittis et. Nam tellus dolor, pulvinar non fermentum quis, auctor a metus. Proin mi purus, dignissim eget tempus in, vestibulum eu libero.

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed. Aliquam et augue est, et pretium est. Aenean mattis lacinia elit quis accumsan.

Nunc pellentesque magna et augue hendrerit dignissim. Donec tempus velit quis purus tincidunt placerat. Suspendisse venenatis aliquet elit id convallis. Praesent porttitor, libero eu pretium gravida, urna velit sollicitudin lectus, sit amet malesuada velit quam tristique diam.

% P2
% 2-col Dependency parse graphic
% Col 1
\begin{table*}[ht]
\centering
    \begin{tabular}{ll|l}
Transition & & Precondition \\
\hline \hline
Left-Arc   & (notation) & (S0 has no head) \\ 
Right-Arc  & (notation) &   \\
Reduce     & (notation) & (S0 has a head) \\  
Shift      & (notation) & \\
\end{tabular}
\caption{In the Arc-Eager transition system, an arc is created either when the word is
    pushed or popped from the stack. There are thus two pairs of operations: (Shift, Left)
    and (Right, Reduce). The choice of pop move is constrained by which push move
    was selected, so that the state is updated monotonically.\\
Instead, we give the model free choice of pop moves, and reverse the previous
decision if necessary. This allows the parser to recover from mistakes.}
\label{tab:transitions}
\end{table*}



\section{Arc-Eager Transition-based Dependency Parsing}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus dapibus. Phasellus elementum eleifend neque. Pellentesque non enim urna, in molestie nisi. Sed mattis orci a massa ultricies congue. Suspendisse euismod cursus dolor, a luctus elit ultrices et. Praesent at eros orci. Integer suscipit neque sit amet felis eleifend vel ullamcorper mauris congue. Sed imperdiet ultricies elit in adipiscing. Quisque condimentum consequat turpis at feugiat. Sed fringilla viverra quam, sed auctor nunc convallis vitae. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Phasellus vitae elit vel quam pulvinar cursus ut nec erat.

Maecenas sed sollicitudin nisl. Maecenas vehicula lacinia tristique. In nec sapien augue, eu tincidunt arcu. Nulla molestie lectus a sapien tincidunt a volutpat nisl consequat. Vivamus consectetur semper risus, nec pretium felis sagittis et. Nam tellus dolor, pulvinar non fermentum quis.

\subsection{Dynamic oracles}

An essential component when training a transition-based parser is an oracle
which, given a gold-standard tree, dictates the sequence of moves a parser
should make in order to derive it.  Traditionally, these oracles are defined
as functions from trees to sequences, mapping a gold tree to a single sequence
of actions deriving it, even if more than one sequence of actions derives the
gold tree. We call such oracles \emph{static}.  Recently, Goldberg and Nivre
\shortcite{coling2012} introduced the concept of a \emph{dynamic} oracle, and
presented a concrete oracle for the arc-eager system.  Instead of mapping a
gold tree to a sequence of actions, the dynamic oracle maps a
\tuple{configuration, tree} pair to a \emph{set} of possible parser actions
which are optimal at the given configuration.  More concretely, the dynamic
oracle presented in \cite{coling2012} maps \tuple{action,configuration,tree}
tuples to an integer, indicating the number of gold arcs in $tree$ that can be
derived from $configuration$ by some sequence of actions, but could not be derived
after applying $action$ to the configuration.\footnote{This correctness of
the oracle is based on a property of the arc-eager system, stating that if a
set of arcs which can be extended to a projective tree can be individually
derived from a given configuration, then a projective tree containing all of
the arcs in the set is also derivable from the same configuration.  This same
property holds also for the transition system we present below.} We refer to
this number as the $cost$ of taking action $a$ at configuration $c$. Actions
with a cost of 0 are optimal at the given configuration, and as long as we
follow only optimal actions we are guaranteed to reach the gold tree.
%By definition, there is at least one 0-cost actions at each configuration.
%Thus, for our purposes we only care to know if the cost of an action is zero
%or non-zero. 

The dynamic oracle for the arc-eager system is based on the properties of the
system by which once words are put on the stack they can acquire heads and
dependents only from the buffer, and once they are removed from the stack they
can no longer acquire any heads or dependents. In addition, once a word
acquired a head it can no longer acquire another head.  Given that words that
are removed from the stack or the buffer can not return there later, it
is straightforward to compute the number of gold arcs that are ``lost'' by an
action by considering the effect of the action on the buffer and the stack,
and reasoning accordingly (the notation assumes the configuration $c$ has $b$
as the first item in the buffer $\beta$ and $s$ is the top of the stack
$\sigma$, and $gold$ is a set of arcs. An arc is a triplet $(m,l,h)$ %TODO verify direction is correct.
indicating that $m$ modifies $h$ with a label $l$).

\begin{itemize}
   \item $\mathcal{C}(\textsc{Left-Arc}_l; c, gold)$: 
Adding the arc $(b, l, s)$ and popping $s$ from the stack means that $s$ will not be able
to acquire any head or dependents in $\beta$. The cost is therefore the number of arcs 
in $gold$ of the form $(k, l', s)$ or $(s, l', k)$ such that $k \in \beta$.

\item $\mathcal{C}(\textsc{Right-Arc}_l; c, gold)$: 
Adding the arc $(s, l, b)$ and pushing $b$ onto the stack means that $b$ will 
not be able to acquire any head in $\sigma$ or $\beta$, nor any dependents in $\sigma$. 
The cost is therefore the number of arcs in $gold$ of the form $(k, l', b)$,
such that $k \in \sigma$ or $k \in \beta$, 
or of the form $(b, l', k)$ such that $k \in \sigma$. 

\item $\mathcal{C}(\textsc{Reduce}; c, gold)$: 
Popping $s$ from the stack means that $s$ will not be able to acquire any dependents
in $b|\beta$. The cost is therefore the number of arcs in $gold$ of the form $(s, l', k)$
such that $k \in b|\beta$. \maybe{While it may seem that a gold arc of the form $(k, l, s)$ should be accounted for as well,
note that a gold arc of that form, if it exists, is already accounted for by a previous (erroneous) \textsc{Right-Arc}$_l$ 
transition when $s$ acquired its head.}

\item $\mathcal{C}(\textsc{Shift}; c, gold)$: 
Pushing $b$ onto the stack means that $b$ will not be able to acquire any head
or dependents in $s|\sigma$. The cost is therefore the number of arcs in $gold$ of 
the form $(k, l', b)$ or $(b, l', k)$ such that $k \in s|\sigma$. 
\end{itemize}
% TODO matt's note about considering only stack items with no assigned head.


% Col 2
\subsection{Following non-gold transitions in training}
\begin{figure}
\centering
    \begin{verbatim}

while not state.is_finished {
  feats = extract(state)
  zero_costs = oracle(state)
  g = predict_from(feats, zero_costs)
  monotonic = check_constraints(state)
  p = predict_from(feats, monotonic)
  if p not in zero_costs {
     update(feats, g, 1)
     update(feats, p, -1)
  }
  transition(state, p)
}
\end{verbatim}
\caption{Dynamic oracle. TODO: Notation.}
\end{figure}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus dapibus. Phasellus elementum eleifend neque. Pellentesque non enim urna, in molestie nisi. Sed mattis orci a massa ultricies congue. Suspendisse euismod cursus dolor, a luctus elit ultrices et. Praesent at eros orci. Integer suscipit neque sit amet felis eleifend vel ullamcorper mauris congue. Sed imperdiet ultricies elit in adipiscing. Quisque condimentum consequat turpis at feugiat. Sed fringilla viverra quam, sed auctor nunc convallis vitae. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Phasellus vitae elit vel quam pulvinar cursus ut nec erat.

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus.

\begin{figure}
    \centering
    \begin{dependency}[theme=simple]
        \tikzstyle{t}=[text=green,ultra thick,font=\bfseries\itshape]
        \tikzstyle{f}=[text=red,ultra thick,font=\bfseries\itshape]
        \tikzstyle{m}=[font=\bfseries\itshape]
        \tikzstyle{n}=[font=\itshape]

        \begin{deptext}[column sep=.075cm, row sep=.1ex]
            I \&           \& saw \&          \& Jack       \& \& and     \&           \& Jill   \&  \& fall \& \textsc{r} \\
|[m]|S \& |[m]|L \& |[m]|S   \& |[f]|R \& |[m]|R \& |[m]|D \& |[m]|R \& |[m]|D \& |[t]|L \& |[n]|R \& |[n]|D \& |[n]|L \\
            1 \&     2       \& 3  \&   4      \& 5          \& 6 \& 7     \& 8 \& 9 \& 10 \& 11 \& 12 \\
    I \&           \& saw \&          \& Jack       \& \& and     \&           \& Jill   \&      \& fall \& \textsc{r} \\
        \end{deptext}
    \depedge{3}{1}{2}
    \depedge[red, ultra thick]{3}{5}{4}
    \depedge{5}{7}{5}
    \depedge{5}{9}{7}
    
    \deproot[edge height=0.7cm, ultra thick]{11}{}
    \wordgroup{1}{3}{3}{}
    \wordgroup{1}{5}{5}{}
    
    \depedge[edge below]{3}{1}{2}
    \depedge[edge below]{5}{7}{5}
    \depedge[edge below]{5}{9}{7}
    \depedge[edge below, green, ultra thick]{11}{5}{9}
    \wordgroup{4}{3}{3}{}
\end{dependency}
\caption{
    \small
    State before and after a non-monotonic Left-Arc.
    At move 9, \emph{fall} is the first word of the buffer (marked with an arrow),
    and \emph{saw} and \emph{Jack} are on the stack (circled). The arc created at move 4 was
    incorrect (in red). Arcs are labelled with the move that created them.
    After move 9 (the lower state), the non-monotonic Left-Arc move
    has replaced the incorrect dependency with a correct Left-Arc (in green).
\label{fig:clobber}}
\end{figure}



\section{The Non-Monotonic Arc-Eager Transition System}

The Arc-Eager transition system \citep{nivre:04} has four moves. Two create 
dependencies, two push the first word of the buffer to the stack, and two pop 
the stack:

\begin{center}
    \begin{tabular}{l|cc}
             & Push  & Pop    \\
           \hline
           Sets head   & \emph{Right} & \textbf{Left}    \\
            No head    & \textbf{Shift} & \emph{Reduce}   \\
     \end{tabular}
\end{center}

Every word in the sentence is pushed once and popped once; and every
word must have exactly one head. This creates two pairings, along the
diagonals: (S, L) and (R, D). Every word must acquire
its head when it is pushed or when it is popped, but not both, and not neither.

The standard approach for guaranteeing this is to constrain the choice of pop move:

\begin{itemize}\setlength{\itemsep}{-2mm}
    \item Iff $S_0$ has a head, Reduce;
    \item Iff $S_0$ has no head, Left-Arc.
\end{itemize}
The pair is therefore decided by the first move in the standard system.
Our idea is simply to let the pair be decided by the \emph{second} move.

This makes the transition system \emph{non-monotonic}, because if the model decides
on an incongruent pairing
we will have to either undo a dependency we now regard as incorrect, or add a
dependency we now regard as missing. We now describe these operations.

\subsection{Non-monotonic Left-Arc}

The upper arcs in Figure \ref{fig:clobber} show a transition history where the wrong
push move was selected. The parser began correctly by Shifting \emph{I}
and Left-Arcing it to \emph{saw}, which was then also Shifted. The incorrect move,
4 (in red), is to Right-Arc \emph{Jack} instead of Shifting it.

The difficulty of this kind of sentence for an incremental parser is fundamental.
The leftward context does not constrain the decision, 
and an arbitrary amount of text could separate \emph{Jack} from its true head,
\emph{fall}. Eye-tracking
experiments show that humans often perform a saccade while reading such examples (TODO).

In moves 5-8 the parser correctly builds the rest of the \textsc{np}, and arrives
at \emph{fall}. The monotonicity constraints would here force an incorrect analysis,
even though the evidence for the correct decision is much stronger at the later move.

We allow Left-Arcs to `clobber' Right-Arcs if the model
recommends it. The Right-Arc is deleted, and the Left-Arc proceeds as normal. The
effect of this is exactly as if the model had correctly chosen Shift at
move 4. We simply give the model a second chance to make the correct choice.

\begin{figure}
    \centering
    \begin{dependency}[theme=simple]
    \tikzstyle{t}=[text=green,ultra thick,font=\bfseries\itshape]
    \tikzstyle{f}=[text=red,ultra thick,font=\bfseries\itshape]
    \tikzstyle{m}=[font=\bfseries\itshape]
    \tikzstyle{n}=[font=\itshape]
    \begin{deptext}[column sep=.075cm, row sep=.1ex]
        I \&           \& saw \&          \& Jack       \& \& and     \&           \& Jill \&   \& \textsc{r} \\
       |[m]|S \& |[m]|L \& |[m]|S   \& |[f]|S \& |[m]|R \& |[m]|D \& |[m]|R \& |[m]|D \& |[m]|R \& |[m]|D \& |[t]|D \& |[n]|L \\
            1 \&     2       \& 3  \&   4      \& 5          \& 6 \& 7     \& 8 \& 9 \& 10 \& 11 \& 12 \\
            I \&           \& saw \&          \& Jack       \& \& and     \&           \& Jill \& \& \textsc{r} \\
\end{deptext}
    \depedge{3}{1}{2}
    \depedge{5}{7}{5}
    \depedge{5}{9}{7}
    
    \deproot[edge height=0.7cm, ultra thick]{11}{}
    \wordgroup{1}{3}{3}{}
    \wordgroup{1}{5}{5}{}
    
    \depedge[edge below]{3}{1}{2}
    \depedge[edge below]{5}{7}{5}
    \depedge[edge below]{5}{9}{7}
    \depedge[edge below, green, ultra thick]{3}{5}{9}
    \wordgroup{4}{3}{3}{}
\end{dependency}
\caption{
\small
State before and after a non-monotonic Reduce.
After making the wrong push move at 4, at move 11
the parser has \emph{Jack} on the stack (circled), with only the dummy \textsc{root}
token left in the buffer. A monotonic parser must deterministically Left-Arc
\emph{Jack} here to preserve the previous decision, despite the current state.
We remove this constraint, and instead assume that when the model selects Reduce
for a headless item, it is reversing the previous Shift/Right decision. We add
the appropriate arc, assigning the label that scored highest when the Shift/Right
decision was originally made.
\label{fig:adduce}}
\end{figure}

\subsection{Non-monotonic Reduce}

The upper arcs in Figure \ref{fig:adduce} show a state resulting from the opposite error.
The parser has Shifted \emph{Jack} instead of Right-Arcing it. After
building the \textsc{np} the buffer is exhausted, except for the artificial root token,
which we use to Left-Arc the head of the sentence, following \citet{nivre:squib}.

Instead of letting the previous choice lock us in to the pair (Shift, Left), we let
the latter decision reverse it to (Right, Reduce). This gives the model the flexibility
to take advantage of the extra information, and make what should be an easy decision.
When the Shift/Right decision is reversed, we add an arc between the top of the stack
and the word above it. This is the arc that would have been created had the parser
chosen to Right-Arc when it chose to Shift. Since our idea is to reverse this mistake,
we select the Right-Arc label that the model scored most highly at that time.


\subsection{Why have two push moves?}

We have argued above that it is better to trust the second decision that the model
makes, rather than using the first decision to determine the second. If this is
the case, then why bother even making a decision on the first move?

We can think of two ways to achieve this. 
In Section \ref{sec:left} we explained how the Left-Arc can be made to correct
Shift/Right confusions.  Using this, we could label all Shifts as Right-Arcs,
while still providing a path to the gold-standard dependencies. Equally, 
in Section \ref{sec:reduce} we showed how the Reduce
move can be extended to correct Right/Shift confusions, so we could instead
remove the Right-Arcs, and Shift every word onto the stack. They would either be
Left-Arced, or Reduced and attached to the word above them on the
stack.

The problem with these approaches is that they do not do a good job
of accumulating information in the parse as it progresses. The structure built by the
original Arc-Eager system will be more informative than the ones built by these
3-move systems. Most transition decisions are easy, and their predictions yield an
informative state that makes the difficult decisions possible.

For instance, removing the Right-Arc move would mean that words are always Shifted,
in order to delay setting their head. However, none of the states in between those two
points would be able to condition on the previous choices. They would miss out on
informative features, such as the 
dependency label of $S_0$, its rightward children, etc.

The non-monotonic transition system that we propose does not have this problem.
The model still makes Shift vs. Right decisions, and conditions on them --- but
without \emph{committing} to them -- the model still has a chance to repair
some previous mistakes when more information about the continuation of the
sentence presents itself.

\subsection{Defining the oracle}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus dapibus. Phasellus elementum eleifend neque. Pellentesque non enim urna, in molestie nisi. Sed mattis orci a massa ultricies congue. Suspendisse euismod cursus dolor, a luctus elit ultrices et. Praesent at eros orci. Integer suscipit neque sit amet felis eleifend vel ullamcorper mauris congue.

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus dapibus. Phasellus elementum eleifend neque. Pellentesque non enim urna, in molestie nisi. Sed mattis orci a massa ultricies congue. Suspendisse euismod cursus dolor, a luctus elit ultrices et. Praesent at eros orci. Integer suscipit neque sit amet felis eleifend vel ullamcorper mauris congue.

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus dapibus. Phasellus elementum eleifend neque. Pellentesque non enim urna, in molestie nisi. Sed mattis orci a massa ultricies congue. Suspendisse euismod cursus dolor, a luctus elit ultrices et. Praesent at eros orci. Integer suscipit neque sit amet felis eleifend vel ullamcorper mauris congue.


Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus dapibus. Phasellus elementum eleifend neque. Pellentesque non enim urna, in molestie nisi. Sed mattis orci a massa ultricies congue. Suspendisse euismod cursus dolor, a luctus elit ultrices et. Praesent at eros orci. Integer suscipit neque sit amet felis eleifend vel ullamcorper mauris congue.

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus dapibus. Phasellus elementum eleifend neque. Pellentesque non enim urna, in molestie nisi. Sed mattis orci a massa ultricies congue. Suspendisse euismod cursus dolor, a luctus elit ultrices et.

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus dapibus. Phasellus elementum eleifend neque. Pellentesque non enim urna, in molestie nisi. Sed mattis orci a massa ultricies congue. Suspendisse euismod cursus dolor, a luctus elit ultrices et. Praesent at eros orci. Integer suscipit neque sit amet felis eleifend vel ullamcorper mauris congue.

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus dapibus. Phasellus elementum eleifend neque. Pellentesque non enim urna, in molestie nisi. Sed mattis orci a massa ultricies congue. Suspendisse euismod cursus dolor, a luctus elit ultrices et. Praesent at eros orci. Integer suscipit neque sit amet felis eleifend vel ullamcorper mauris congue.
% P5
\section{Experiments}

We implemented a standard Arc-Eager parser trained with the averaged Perceptron,
and used it to evaluate the non-monotonic transitions. We tested the transitions on
multiple dependency schemes, to check whether any effects were specific to annotation
contingencies. We also analyse the effect on accuracy by dependency type, and
investigate the impact on error propagation.


\begin{table}[t]
    \small
    \centering
    \begin{tabular}{l|rrrr}
        \hline
                              & Stanford & MALT & CoNLL & ?? \\
        \hline \hline
                              & \multicolumn{4}{c}{Unlabelled Attachment Scores} \\
        \hline
        Baseline              &  00.0  & 00.0 & 00.0 & 00.0   \\
        NM L                  &  00.0  & 00.0 & 00.0 & 00.0   \\
        NM D                  &  00.0  & 00.0 & 00.0 & 00.0   \\
        NM L+D                &  00.0  & 00.0 & 00.0 & 00.0    \\ 
        \hline
                              & \multicolumn{4}{c}{Labelled Attachment Score} \\
        \hline
        Baseline              &  00.0  & 00.0 & 00.0  & 00.0   \\
        NM L                  &  00.0  & 00.0 & 00.0  & 00.0   \\
        NM D                  &  00.0  & 00.0 & 00.0  & 00.0   \\
        NM L+D                &  00.0  & 00.0 & 00.0  & 00.0   \\ 
        \hline
    \end{tabular}
    \caption{\small Effect of non-monotonic transitions with \textbf{standard training}.
             All results averaged across 20 runs with different random seeds. All models
             had $\sigma$=0.07-0.08.\label{tab:standard}}
\end{table}


\subsection{Data and Evaluation}

Nunc pellentesque magna et augue hendrerit dignissim. Donec tempus velit quis purus tincidunt placerat. Suspendisse venenatis aliquet elit id convallis. Praesent porttitor, libero eu pretium gravida, urna velit sollicitudin lectus, sit amet malesuada velit quam tristique diam. Nunc sem mauris, volutpat nec rhoncus quis, sodales ut massa. Praesent egestas ante interdum elit laoreet volutpat. Morbi quis velit tempor libero consequat volutpat at vel nulla. Morbi et justo ac liberos.
% P6

\subsection{Parsing model}

We base our experiments on the parser described by \citet{goldberg:12}. We
began by implementing their baseline system, a standard Arc-Eager parser using
an averaged Perceptron learner and the extended feature set described by \citet{zhang:11}.
We follow \citeauthor{goldberg:12} in training all models for 15 iterations.

Labels and moves were learned jointly, resulting in a single model with 88 classes
for the Stanford dependencies. Joint label/move learning proved surprisingly
advantageous, giving 0.4\% additional \uas.

We follow the suggestion of \citet{nivre:squib} to handle root-node dependencies by
adding a dummy token at the \emph{end} of the sentence.
This improves accuracy by delaying root-node decisions, and likely accounts for the
0.2\% extra accuracy of our standard-trained baseline system over the equivalent result
reported by \citet{goldberg:12}. The only other known difference between the systems
is a modest model compaction step performed by the proprietary Google perceptron
code.

\subsection{Training strategies}

\textbf{Standard.} Nunc pellentesque magna et augue hendrerit dignissim. Donec tempus velit quis purus tincidunt placerat. Suspendisse venenatis aliquet elit id convallis. Praesent porttitor, libero eu pretium gravida, urna velit sollicitudin lectus, sit amet malesuada velit quam tristique diam. Nunc sem mauris, volutpat nec rhoncus quis,

\textbf{Goldberg and Nivre.} Nunc pellentesque magna et augue hendrerit dignissim. Donec tempus velit quis purus tincidunt placerat. Suspendisse venenatis aliquet elit id convallis. Praesent porttitor, libero eu pretium gravida.


\begin{table}[t]
    \small
    \centering
    \begin{tabular}{l|rrrr}
        \hline
                              & Stanford & MALT & CoNLL & ?? \\
        \hline \hline
                              & \multicolumn{4}{c}{Unlabelled Attachment Scores} \\
        \hline
        Baseline              &  00.0  & 00.0 & 00.0 & 00.0   \\
        NM L                  &  00.0  & 00.0 & 00.0 & 00.0   \\
        NM D                  &  00.0  & 00.0 & 00.0 & 00.0   \\
        NM L+D                &  00.0  & 00.0 & 00.0 & 00.0    \\ 
        \hline
                              & \multicolumn{4}{c}{Labelled Attachment Score} \\
        \hline
        Baseline              &  00.0  & 00.0 & 00.0  & 00.0   \\
        NM L                  &  00.0  & 00.0 & 00.0  & 00.0   \\
        NM D                  &  00.0  & 00.0 & 00.0  & 00.0   \\
        NM L+D                &  00.0  & 00.0 & 00.0  & 00.0   \\ 
        \hline
    \end{tabular}
    \caption{\small Effect of non-monotonic transitions with \textbf{Goldberg and Nivre training}
            All results averaged across 20 runs with different random seeds. All models
             had $\sigma$=0.07-0.08.\label{tab:goldberg}}
\end{table}

\subsection{Transition systems}

\textbf{Baseline.} Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus dapibus. Phasellus elementum eleifend neque. Pellentesque non enim urna, in molestie nisi. Sed mattis orci a massa ultricies congue. Suspendisse euismod cursus dolor.

\textbf{NM L.} Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue.

\textbf{NM D.} Non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor.

\textbf{NM L+D.} Non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.


\section{Development Results}
\label{sec:results}

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.


\begin{table}
\centering
\small
\begin{tabular}{lrrrr}
        & Freq. & Base & NM & WD \\
    \hline \hline
    \multicolumn{5}{c}{Left-Arcs} \\
    \hline
    det	& 3350 & 00.0 & +0.0 & 0.0 \\
     nn	& 3208 & 00.0 &	-0.0 & 0.0 \\
  nsubj	& 2691 & 00.0 &	0.0 & 0.0 \\
   amod	& 2428 & 00.0 & 0.0 & 0.0 \\
aux	    & 1229 & 00.0 &	0.0 & 0.0 \\
advmod  & 810  & 00.0 & 0.0 & 0.0 \\
num	    & 749  & 00.0 & 0.0 & 0.0 \\
   poss	& 707  & 00.0 & 0.0 & 0.0 \\
  Other	& 2637 & 00.0 & 0.0 & 0.0 \\
\hline
\multicolumn{5}{c}{Right-Arcs} \\
\hline
pobj	& 3739  & 00.0 & 0.0 & 0.0 \\
prep	& 3498  & 00.0 & 0.0 & 0.0 \\
dobj	& 1568  & 00.0 & 0.0 & 0.0 \\
conj	& 1002  & 00.0 & 0.0 & 0.0 \\
cc	    & 898   & 00.0 & 0.0 & 0.0 \\
number	& 468   & 00.0 & 0.0 & 0.0 \\
 ccomp	& 449   & 00.0 & 0.0 & 0.0 \\
 xcomp	& 438   & 00.0 & 0.0 & 0.0 \\
advmod	& 437   & 00.0 & 0.0 & 0.0 \\
ps	    & 425   & 00.0 & 0.0 & 0.0 \\
dep	    & 406   & 00.0 & 0.0 & 0.0 \\
  Other	& 2549  & 00.0 & 0.0 & 0.0 \\
\hline
All left  & 000 & 00.0 & 0.0 & 0.0  \\
All right & 000 & 00.0 & 0.0 & 0.0  \\
Root    & 1700 & 00.0 & 0.0 & 0.0    \\     
\hline
\end{tabular}
\caption{\small Accuracies for common Stanford labels,
         using G\&N training and standard features.
         \textbf{NM L+D} accuracy is shown by difference from the baseline. The
         WD column shows the difference weighted by the frequency of the label.
     \label{tab:labels}}
\end{table}


\subsection{Accuracy by label}

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.


\subsection{Effect on error propagation}

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.

\begin{table}
    \small
    \centering
    \begin{tabular}{l|rrr|rrr}
        & \multicolumn{3}{c|}{Standard Training} & \multicolumn{3}{c}{G\&N Training}\\
        $E$   & N & Baseline & NM    & N &Baseline & NM    \\
        \hline \hline
        0     &   &          &        &   &       &        \\
        1     &   &          &        &   &       &        \\
        2     &   &          &        &   &       &        \\
        3     &   &          &        &   &       &        \\
        4     &   &          &        &   &       &        \\
        5     &   &          &        &   &       &        \\
        $>5$  &   &          &        &   &       &        \\
        \hline
    \end{tabular}
\caption{\small Likelihood of making a transition error in a state given $E$ previous
         errors with the two training strategies. The system accuracy (NM L+D) is
     shown as a difference from the baseline.\label{tab:errprop}}
\end{table}

\subsection{Interaction with additional features}

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.
\begin{table}
\small

\centering
(S1w), (S1p), (S1w, S1p),\\
(S1w, N0w), (S1w, N0p), (S1p, N0p),\\
(S1w, S0w, N0w), (S1p, S0p, N0p), (S1p, S0p, N0p),\\
(S2w, N0w),\\
(S2w, N1w),\\
(S2p, N0p, N1w), (S2p, N0w, N1w), (S2w, N0p, N1p), \\
(S3w, N0w), \\
(S2w, N1w),\\ 
(S3p, S2p, N0w),\\
(dist, S1w, N1w),\\
(dist, S1p, N0p, N1p)
\caption{Additional features. S=Stack, N=buffer. w=Word, p=\pos. Dist=Distance
     between S0 and N0. TODO: Prettify this. TODO: Update it, it's from the old version.}
\end{table}

\begin{table}
\centering
\begin{tabular}{lrr}
    \hline 
            & \las  & \uas  \\
    \hline \hline 
                & \multicolumn{2}{c}{Standard Features} \\
                \hline
    Baseline    & 00.0 & 00.0 \\
    NM L+D      & 00.0 & 00.0 \\
\hline
                & \multicolumn{2}{c}{Additional features} \\
                \hline
    Baseline    & 00.0 & 00.0 \\
    NM L+D      & 00.0 & 00.0 \\
\end{tabular}
\caption{
    \small
    Impact of the additional features on Stanford \wsj22 using the baseline and
    non-monotonic transition systems.
\label{tab:feats}}
\end{table}


\section{Test Results}

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.


\begin{table}
    \centering
    \small
    \begin{tabular}{l|r|rr|rr}
        \hline 
System  &   $O$      & \multicolumn{2}{c|}{Penn2Malt} & \multicolumn{2}{c}{Stanford} \\
                           &          & \las  & \uas  & \las & \uas \\
        \hline \hline
K\&C 10  & $n^3$ & ---   & 93.00 & ---  & --- \\
Z\&N 11  & $nk$  & 91.8  & 92.9  & 91.9 & 93.5\\
G\&N 12  & $n$   & ---   & ---   & 88.72 & 90.96 \\
        \hline
Baseline    & $n$ &       &       &       &  \\
NM L+D & $n$ &       &       &       &  \\
\hline
Base + Feats   & $n$ &       &       &       &  \\
NM L+D + Feats & $n$ &       &       &       &  \\
\hline
    \end{tabular}
    \caption{Test results on \wsj 23. 
         \label{tab:feats}}
\end{table}



% P8
\section{Related Work}


Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.

\subsection{Parse restructuring in psycholinguistics}

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.

\section{Conclusion}

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.

\bibliography{repair}
\bibliographystyle{aclnat}


\end{document}
