% vim: set textwidth=78 fo+=t :

\documentclass[11pt,letterpaper]{article}
\usepackage{acl2013}
\usepackage{amsmath, amsthm}
\usepackage{times}
\usepackage{latexsym}
\usepackage{xspace}
\usepackage{natbib}
\usepackage{tikz-dependency}
\usepackage{placeins}
\usepackage{xcolor}
% Alter some LaTeX defaults for better treatment of figures:
    % See p.105 of "TeX Unbound" for suggested values.
    % See pp. 199-200 of Lamport's "LaTeX" book for details.
    %   General parameters, for ALL pages:
    \renewcommand{\topfraction}{0.9}	% max fraction of floats at top
    \renewcommand{\bottomfraction}{0.8}	% max fraction of floats at bottom
    %   Parameters for TEXT pages (not float pages):
    \setcounter{topnumber}{2}
    \setcounter{bottomnumber}{2}
    \setcounter{totalnumber}{4}     % 2 may work better
    \setcounter{dbltopnumber}{2}    % for 2-column pages
    \renewcommand{\dbltopfraction}{0.9}	% fit big float above 2-col. text
    \renewcommand{\textfraction}{0.07}	% allow minimal text w. figs
    %   Parameters for FLOAT pages (not text pages):
    \renewcommand{\floatpagefraction}{0.7}	% require fuller float pages
	% N.B.: floatpagefraction MUST be less than topfraction !!
    \renewcommand{\dblfloatpagefraction}{0.7}	% require fuller float pages

	% remember to use [htp] or [htpb] for placement


\setlength\titlebox{6.5cm}    % Expanding the titlebox


\renewcommand{\tabcolsep}{5pt}

\newcommand{\baseacc}{00.00\xspace}
\newcommand{\sysacc}{00.00\xspace}
\newcommand{\sysimprove}{00.00\xspace}
\newcommand{\las}{\textsc{las}\xspace}
\newcommand{\uas}{\textsc{uas}\xspace}
\newcommand{\pp}{\textsc{pp}\xspace}
\newcommand{\pos}{\textsc{pos}\xspace}
\newcommand{\wsj}{\textsc{wsj}\xspace}

\newcommand{\stacktop}{S$_0$\xspace}
\newcommand{\buffone}{N$_0$\xspace}

\newcommand{\tuple}[1]{$\langle#1\rangle$}
\newcommand{\maybe}[1]{\textcolor{gray}{#1}}
\newcommand{\note}[1]{\textcolor{red}{#1}}

\title{A Non-Monotonic Arc-Eager Transition System for Dependency Parsing}

\author{Author 1\\
	    XYZ Company\\
	    111 Anywhere Street\\
	    Mytown, NY 10000, USA\\
	    {\tt author1@xyz.org}
	  \And
	Author 2\\
  	ABC University\\
  	900 Main Street\\
  	Ourcity, PQ, Canada A1A 1T2\\
  {\tt author2@abc.ca}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
    Previous incremental parsers have assumed that state transitions
    should be monotonic. However, transitions can be made to revise
    previous decisions quite naturally, based on further information.
    Constraining this behaviour amounts
    to placing less trust in the model, which we can expect to be less productive as
    our models improve.

    We show how simple adjustments to the Arc-Eager transition system to relax the
    monotonicity constraint improves accuracy, especially when imperfect
    states are introduced during training. To support our claim that better models
    profit more from non-monotonic transitions, we present a set of
    additional features that improve accuracy substantially. Despite having fewer errors
    to correct, relaxing the monotonicity constraints is even more productive on
    the improved model, for a total improvement on the previous state-of-the-art of
    Y\%.


    %Previous algorithms for incremental parsing have assumed that state transititions
    %should be monotonic. However, psycholinguistic models of sentence processing
    %typically include moves that restructure the parse tree, in order to account
    %for eye-tracking and reading time experiments.

    %We present a non-monotonic transition system
    %that achieves state-of-the-art accuracy for greedy English dependency parsing.
    %We show that simply removing standard constraints on the Arc-Eager system
    %leads to useful non-monotonic behaviour, especially when imperfect states
    %are introduced during training. With the addition of a new transition
    %to correct attachment errors, the repair moves improve accuracy by 0.5\% on
    %the standard evaluation, over a state-of-the-art baseline.
\end{abstract}

% P1
\section{Introduction}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus dapibus. Phasellus elementum eleifend neque. Pellentesque non enim urna, in molestie nisi. Sed mattis orci a massa ultricies congue. Suspendisse euismod cursus dolor, a luctus elit ultrices et. Praesent at eros orci. Integer suscipit neque sit amet felis eleifend vel ullamcorper mauris congue. Sed imperdiet ultricies elit in adipiscing. Quisque condimentum consequat turpis at feugiat. Sed fringilla viverra quam, sed auctor nunc convallis vitae. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Phasellus vitae elit vel quam pulvinar cursus ut nec erat.

Maecenas sed sollicitudin nisl. Maecenas vehicula lacinia tristique. In nec sapien augue, eu tincidunt arcu. Nulla molestie lectus a sapien tincidunt a volutpat nisl consequat. Vivamus consectetur semper risus, nec pretium felis sagittis et. Nam tellus dolor, pulvinar non fermentum quis, auctor a metus. Proin mi purus, dignissim eget tempus in, vestibulum eu libero.

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed. Aliquam et augue est, et pretium est. Aenean mattis lacinia elit quis accumsan.

Nunc pellentesque magna et augue hendrerit dignissim. Donec tempus velit quis purus tincidunt placerat. Suspendisse venenatis aliquet elit id convallis. Praesent porttitor, libero eu pretium gravida, urna velit sollicitudin lectus, sit amet malesuada velit quam tristique diam.

% P2
% 2-col Dependency parse graphic
% Col 1
\begin{table*}[ht]
\centering
    \begin{tabular}{ll|l}
Transition & & Precondition \\
\hline \hline
Left-Arc   & (notation) & (S0 has no head) \\ 
Right-Arc  & (notation) &   \\
Reduce     & (notation) & (S0 has a head) \\  
Shift      & (notation) & \\
\end{tabular}
\caption{In the Arc-Eager transition system, an arc is created either when the word is
    pushed or popped from the stack. There are thus two pairs of operations: (Shift, Left)
    and (Right, Reduce). The choice of pop move is constrained by which push move
    was selected, so that the state is updated monotonically.\\
Instead, we give the model free choice of pop moves, and reverse the previous
decision if necessary. This allows the parser to recover from mistakes.}
\label{tab:transitions}
\end{table*}



\section{Arc-Eager Transition-based Dependency Parsing}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus dapibus. Phasellus elementum eleifend neque. Pellentesque non enim urna, in molestie nisi. Sed mattis orci a massa ultricies congue. Suspendisse euismod cursus dolor, a luctus elit ultrices et. Praesent at eros orci. Integer suscipit neque sit amet felis eleifend vel ullamcorper mauris congue. Sed imperdiet ultricies elit in adipiscing. Quisque condimentum consequat turpis at feugiat. Sed fringilla viverra quam, sed auctor nunc convallis vitae. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Phasellus vitae elit vel quam pulvinar cursus ut nec erat.

Maecenas sed sollicitudin nisl. Maecenas vehicula lacinia tristique. In nec sapien augue, eu tincidunt arcu. Nulla molestie lectus a sapien tincidunt a volutpat nisl consequat. Vivamus consectetur semper risus, nec pretium felis sagittis et. Nam tellus dolor, pulvinar non fermentum quis.


% Col 2
\subsection{Following non-gold transitions in training}
\begin{figure}
\centering
    \begin{verbatim}

while not state.is_finished {
  feats = extract(state)
  zero_costs = oracle(state)
  g = predict_from(feats, zero_costs)
  monotonic = check_constraints(state)
  p = predict_from(feats, monotonic)
  if p not in zero_costs {
     update(feats, g, 1)
     update(feats, p, -1)
  }
  transition(state, p)
}
\end{verbatim}
\caption{Dynamic oracle. TODO: Notation.}
\end{figure}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus dapibus. Phasellus elementum eleifend neque. Pellentesque non enim urna, in molestie nisi. Sed mattis orci a massa ultricies congue. Suspendisse euismod cursus dolor, a luctus elit ultrices et. Praesent at eros orci. Integer suscipit neque sit amet felis eleifend vel ullamcorper mauris congue. Sed imperdiet ultricies elit in adipiscing. Quisque condimentum consequat turpis at feugiat. Sed fringilla viverra quam, sed auctor nunc convallis vitae. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Phasellus vitae elit vel quam pulvinar cursus ut nec erat.

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus.

\begin{figure}
    \centering
    \begin{dependency}[theme=simple]
        \tikzstyle{t}=[text=green,ultra thick,font=\bfseries\itshape]
        \tikzstyle{f}=[text=red,ultra thick,font=\bfseries\itshape]
        \tikzstyle{m}=[font=\bfseries\itshape]
        \tikzstyle{n}=[font=\itshape]

        \begin{deptext}[column sep=.075cm, row sep=.1ex]
            I \&           \& saw \&          \& Jack       \& \& and     \&           \& Jill   \&  \& fall \& \textsc{r} \\
|[m]|S \& |[m]|L \& |[m]|S   \& |[f]|R \& |[m]|R \& |[m]|D \& |[m]|R \& |[m]|D \& |[t]|L \& |[n]|R \& |[n]|D \& |[n]|L \\
            1 \&     2       \& 3  \&   4      \& 5          \& 6 \& 7     \& 8 \& 9 \& 10 \& 11 \& 12 \\
    I \&           \& saw \&          \& Jack       \& \& and     \&           \& Jill   \&      \& fall \& \textsc{r} \\
        \end{deptext}
    \depedge{3}{1}{2}
    \depedge[red, ultra thick]{3}{5}{4}
    \depedge{5}{7}{5}
    \depedge{5}{9}{7}
    
    \deproot[edge height=0.7cm, ultra thick]{11}{}
    \wordgroup{1}{3}{3}{}
    \wordgroup{1}{5}{5}{}
    
    \depedge[edge below]{3}{1}{2}
    \depedge[edge below]{5}{7}{5}
    \depedge[edge below]{5}{9}{7}
    \depedge[edge below, green, ultra thick]{11}{5}{9}
    \wordgroup{4}{3}{3}{}
\end{dependency}
\caption{
    \small
    State before and after a non-monotonic Left-Arc.
    At move 9, \emph{fall} is the first word of the buffer (marked with an arrow),
    and \emph{saw} and \emph{Jack} are on the stack (circled). The arc created at move 4 was
    incorrect (in red). Arcs are labelled with the move that created them.
    After move 9 (the lower state), the non-monotonic Left-Arc move
    has replaced the incorrect dependency with a correct Left-Arc (in green).
\label{fig:clobber}}
\end{figure}



\section{The Non-Monotonic Arc-Eager Transition System}

The Arc-Eager transition system \citep{nivre:04} has four moves. Two create 
dependencies, two push the first word of the buffer to the stack, and two pop 
the stack:

\begin{center}
    \begin{tabular}{l|cc}
             & Push  & Pop    \\
           \hline
           Adds head   & \emph{Right} & \textbf{Left}    \\
            No head    & \textbf{Shift} & \emph{Reduce}   \\
     \end{tabular}
\end{center}

Every word in the sentence is pushed once and popped once; and every
word must have exactly one head. This creates two pairings, along the
diagonals: (S, L) and (R, D).
Either the push move adds the head or the pop move does, but not both and not neither.
The Arc-Eager system guarantees this by constraining the choice of pop move:

\begin{itemize}\setlength{\itemsep}{-2mm}
    \item Iff $S_0$ has a head, Reduce;
    \item Iff $S_0$ has no head, Left-Arc.
\end{itemize}
In the existing solution, the first move decides both.
Our idea is to let the pair be decided by the \emph{second} move.

This makes the transition system \emph{non-monotonic}, because if the model decides
on an incongruent pairing
we will have to either undo or add a
dependency, depending on whether we correct a Right-Arc with a Left-Arc, or correct
a Shift with a Reduce. We now describe these operations.

\subsection{Non-monotonic Left-Arc}

The upper arcs in Figure \ref{fig:clobber} show a transition history where the wrong
push move was selected. The parser began correctly by Shifting \emph{I}
and Left-Arcing it to \emph{saw}, which was then also Shifted. The incorrect move,
4 (in red), is to Right-Arc \emph{Jack} instead of Shifting it.

The difficulty of this kind of sentence for an incremental parser is fundamental.
The leftward context does not constrain the decision, 
and an arbitrary amount of text could separate \emph{Jack} from its true head,
\emph{fall}. Eye-tracking
experiments show that humans often perform a saccade while reading such examples (TODO).

In moves 5-8 the parser correctly builds the rest of the \textsc{np}, and arrives
at \emph{fall}. The monotonicity constraints would here force an incorrect analysis,
even though the evidence for the correct decision is much stronger at the later move.

We allow Left-Arcs to `clobber' Right-Arcs if the model
recommends it. The Right-Arc is deleted, and the Left-Arc proceeds as normal. The
effect of this is exactly as if the model had correctly chosen Shift at
move 4. We simply give the model a second chance to make the correct choice.

\begin{figure}
    \centering
    \begin{dependency}[theme=simple]
    \tikzstyle{t}=[text=green,ultra thick,font=\bfseries\itshape]
    \tikzstyle{f}=[text=red,ultra thick,font=\bfseries\itshape]
    \tikzstyle{m}=[font=\bfseries\itshape]
    \tikzstyle{n}=[font=\itshape]
    \begin{deptext}[column sep=.075cm, row sep=.1ex]
        I \&           \& saw \&          \& Jack       \& \& and     \&           \& Jill \&   \& \textsc{r} \\
       |[m]|S \& |[m]|L \& |[m]|S   \& |[f]|S \& |[m]|R \& |[m]|D \& |[m]|R \& |[m]|D \& |[m]|R \& |[m]|D \& |[t]|D \& |[n]|L \\
            1 \&     2       \& 3  \&   4      \& 5          \& 6 \& 7     \& 8 \& 9 \& 10 \& 11 \& 12 \\
            I \&           \& saw \&          \& Jack       \& \& and     \&           \& Jill \& \& \textsc{r} \\
\end{deptext}
    \depedge{3}{1}{2}
    \depedge{5}{7}{5}
    \depedge{5}{9}{7}
    
    \deproot[edge height=0.7cm, ultra thick]{11}{}
    \wordgroup{1}{3}{3}{}
    \wordgroup{1}{5}{5}{}
    
    \depedge[edge below]{3}{1}{2}
    \depedge[edge below]{5}{7}{5}
    \depedge[edge below]{5}{9}{7}
    \depedge[edge below, green, ultra thick]{3}{5}{9}
    \wordgroup{4}{3}{3}{}
\end{dependency}
\caption{
\small
State before and after a non-monotonic Reduce.
After making the wrong push move at 4, at move 11
the parser has \emph{Jack} on the stack (circled), with only the dummy \textsc{root}
token left in the buffer. A monotonic parser must deterministically Left-Arc
\emph{Jack} here to preserve the previous decision, despite the current state.
We remove this constraint, and instead assume that when the model selects Reduce
for a headless item, it is reversing the previous Shift/Right decision. We add
the appropriate arc, assigning the label that scored highest when the Shift/Right
decision was originally made.
\label{fig:adduce}}
\end{figure}

\subsection{Non-monotonic Reduce}

The upper arcs in Figure \ref{fig:adduce} show a state resulting from the opposite error.
The parser has Shifted \emph{Jack} instead of Right-Arcing it. After
building the \textsc{np} the buffer is exhausted, except for the artificial root token,
which we use to Left-Arc the head of the sentence, following \citet{nivre:squib}.

Instead of letting the previous choice lock us in to the pair (Shift, Left), we let
the latter decision reverse it to (Right, Reduce). This gives the model the flexibility
to take advantage of the extra information, and make what should be an easy decision.
When the Shift/Right decision is reversed, we add an arc between the top of the stack
and the word above it. This is the arc that would have been created had the parser
chosen to Right-Arc when it chose to Shift. Since our idea is to reverse this mistake,
we select the Right-Arc label that the model scored most highly at that time.


\subsection{Why have two push moves?}

We have argued above that it is better to trust the second decision that the model
makes, rather than using the first decision to determine the second. If this is
the case, is the first decision entirely redundant?
Instead of defining how pop moves can correct Shift/Right mistakes, we could
instead eliminate the ambiguity. There are two possibilities:
Shift every token, and create all Right-Arcs via Reduce; or Right-Arc every token,
and override the incorrect arcs with subsequent Left-Arcs.

The problem with these approaches is that it is useful to condition
on the results of previous decisions. Saving all of the difficulty for later
is not a very good structured prediction strategy. Most of the transition decisions are
easy, and when the parser does encounter a difficult decision, it is advantageous
to have the dependency structure as a summary of the previous information, in
much the same way that the previous tags are critical for \pos tagging. 

As an example of the problem, if the Shift move is
eliminated, about half of the Right-Arcs created will be spurious. All of these
arcs will be assigned labels\footnote{If the parser is trained to label the temporary
arcs differently, its action is roughly (but not fully) isomorphic to the Shift move.},
making important features uselessly noisy.

The non-monotonic transition system that we propose does not have this problem.
The model makes Shift vs. Right decisions as normal, and conditions on them --- but
without \emph{committing} to them.


\section{Dynamic oracles}

An essential component when training a transition-based parser is an oracle
which, given a gold-standard tree, dictates the sequence of moves a parser
should make in order to derive it.  Traditionally, these oracles are defined
as functions from trees to sequences, mapping a gold tree to a single sequence
of actions deriving it, even if more than one sequence of actions derives the
gold tree. We call such oracles \emph{static}.  Recently, Goldberg and Nivre
\shortcite{coling2012} introduced the concept of a \emph{dynamic} oracle, and
presented a concrete oracle for the arc-eager system.  Instead of mapping a
gold tree to a sequence of actions, the dynamic oracle maps a
\tuple{configuration, tree} pair to a \emph{set} of possible parser actions
which are optimal at the given configuration.  More concretely, the dynamic
oracle presented in \cite{coling2012} maps \tuple{action,configuration,tree}
tuples to an integer, indicating the number of gold arcs in $tree$ that can be
derived from $configuration$ by some sequence of actions, but could not be derived
after applying $action$ to the configuration.\footnote{This correctness of
the oracle is based on a property of the arc-eager system, stating that if a
set of arcs which can be extended to a projective tree can be individually
derived from a given configuration, then a projective tree containing all of
the arcs in the set is also derivable from the same configuration.  This same
property holds also for the transition system we present below.} We refer to
this number as the $cost$ of taking action $a$ at configuration $c$. Actions
with a cost of 0 are optimal at the given configuration, and as long as we
follow only optimal actions we are guaranteed to reach the gold tree.
%By definition, there is at least one 0-cost actions at each configuration.
%Thus, for our purposes we only care to know if the cost of an action is zero
%or non-zero. 
\subsection{Encoding Spurious Ambiguity}
\note{I'm not sure where to include this text}
An important benefit of dynamic oracles over static ones is that they do not
commit to a single sequence of actions for a deriving a given tree, but instead
compactly encode all the deriving sequences. A suitable training algorithm
(see Section \ref{training}) can then be used to learn a good sequence of
actions based on the oracle's guidance.  The task of dealing with spurious
ambiguity is thus moved from the definition of the oracle (a manual task) to
the training procedure.

While spurious ambiguity in the arc-eager transition system is fairly limited, 
it is a crucial aspect of our non-monotonic system.  The non-monotonic
actions are repair-operations, undoing previous decisions.  It would be very
hard to manually design a heuristic dictating which arcs to construct just so
that they can be removed later, and which arcs not to construct in the first
place.  The dynamic-oracle allows us to encode both options, and let the
learner chose when to take each course. 

\subsection{Training to recover from mistakes}
\note{Do we need this? here?}
Another benefit of the dynamic-oracle as defined here is that it is well
defined for every parser configuration, not just configurations leading to the
gold tree.  In a configuration which cannot lead to the gold tree, the oracle
encodes the paths leading to the best tree which is still reachable.  As we
know the parser is going to make mistakes, it is useful to let it see
configurations that result from common mistakes, and teach it the optimal way
to proceed from them.


\subsection{Dynamic-oracle for the Arc-Eager System}

The dynamic oracle for the arc-eager system is based on the properties of the
system by which once words are put on the stack they can acquire heads and
dependents only from the buffer, and once they are removed from the stack they
can no longer acquire any heads or dependents. In addition, once a word
acquired a head it can no longer acquire another head.  Given that words that
are removed from the stack or the buffer can not return there later, it
is straightforward to compute the number of gold arcs that are ``lost'' by an
action by considering the effect of the action on the buffer and the stack,
and reasoning accordingly (the notation assumes the configuration $c$ has $b$
as the first item in the buffer $\beta$ and $s$ is the top of the stack
$\sigma$, and $gold$ is a set of arcs. An arc is a triplet $(h,l,m)$
indicating that $m$ modifies $h$ with a label $l$).

\begin{itemize}
   \item $\mathcal{C}(\textsc{Left-Arc}_l; c, gold)$: 
Adding the arc $(b, l, s)$ and popping $s$ from the stack means that $s$ will not be able
to acquire any head or dependents in $\beta$. The cost is therefore the number of arcs 
in $gold$ of the form $(k, l', s)$ or $(s, l', k)$ such that $k \in \beta$.

\item $\mathcal{C}(\textsc{Right-Arc}_l; c, gold)$: 
Adding the arc $(s, l, b)$ and pushing $b$ onto the stack means that $b$ will 
not be able to acquire any head in $\sigma$ or $\beta$, nor any dependents in $\sigma$. 
The cost is therefore the number of arcs in $gold$ of the form $(k, l', b)$,
such that $k \in \sigma$ or $k \in \beta$, 
or of the form $(b, l', k)$ such that $k \in \sigma$ 
% the following is new according to matt's fix:
and $k$ is not assigned a head.

\item $\mathcal{C}(\textsc{Reduce}; c, gold)$: 
Popping $s$ from the stack means that $s$ will not be able to acquire any dependents
in $b|\beta$. The cost is therefore the number of arcs in $gold$ of the form $(s, l', k)$
such that $k \in b|\beta$. \maybe{While it may seem that a gold arc of the form $(k, l, s)$ should be accounted for as well,
note that a gold arc of that form, if it exists, is already accounted for by a previous (erroneous) \textsc{Right-Arc}$_l$ 
transition when $s$ acquired its head.}

\item $\mathcal{C}(\textsc{Shift}; c, gold)$: 
Pushing $b$ onto the stack means that $b$ will not be able to acquire any head
or dependents in $s|\sigma$. The cost is therefore the number of arcs in $gold$ of 
the form $(k, l', b)$ such that $k \in s|\sigma$ and of the form $(b, l', k)$ such that $k \in s|\sigma$
% the following is new according to matt's fix:
and $k$ is not assigned a head.
\note{Matthew, notice that shift and right-arc changed their definition, as
you pointed out they should, by requiring the stack item to not have a head
assigned already. The deltas below are based on this new and correct
definitions.}
\end{itemize}

\subsection{Dynamic oracle for the non-monotnic system}

The new system .. like the old one ..
.. with the following changes:
\begin{itemize}
   \item Once a word has acquired a head to its right it can still acquire a
      different head to its left.  (due to change of the Left-arc transition).
   \item Once words are put on the stack they can acquire heads not only from
      the buffer but \textit{also from the word just before them on the
      stack.} (due to the changed semantics of the reduce transition).
\end{itemize}

We will define the new dynamic oracles $\mathcal{C}_{NML}$,
$\mathcal{C}_{NMD}$ and $\mathcal{C}_{NML+D}$ based on the arc-eager dynamic
oracle $\mathcal{C}$ defined above, with additional $\Delta$ terms:

\noindent$\mathcal{C}_{NML}(a,c,t) = \mathcal{C}(a,c,t) + \Delta_{NML}(a,c,t)$\\

\noindent$\mathcal{C}_{NMD}(a,c,t) = \mathcal{C}(a,c,t) + \Delta_{NMD}(a,c,t)$\\

\noindent$\mathcal{C}_{NML+D}(a,c,t) = \mathcal{C}(a,c,t) + \Delta_{NML}(a,c,t) + \Delta_{NMD}(a,c,t)$\\

The terms $\Delta_{NML}$ and $\Delta_{NMD}$ reflect the score adjustments that
need to be done to the arc-eager oracle due to the changes of the Left-Arc and
Reduce actions, respectively.

The change due to the non-monotonic Left-Arc action:

\begin{itemize}
   \item $\Delta_{NML}(\textsc{RightArc},c,gold)$: The cost of Right-arc is decreased by 1 if the gold head of $b$ is on the buffer
(because $b$ can still acquire its correct head later with a Left-Arc action).
It is increased by 1 for any word $w$ on the stack such that $b$ is the gold
parent of $w$ and $w$ is assigned a head already.

   \item $\Delta_{NML}(\textsc{Reduce},c,gold)$: The cost of Reduce is increased by 1 if the gold head of $s$ is on the buffer
(because this will preclude $s$ from acquiring its correct head later on with
a Left-Arc action).

   \item $\Delta_{NML}(\textsc{LeftArc},c,gold)$: The cost of Left-Arc is increased by 1 if $s$ is already assigned to its gold
parent.

  \item $\Delta_{NML}(\textsc{Shift},c,gold)$: The cost of Shift is increased by 1 for any word $w$ on the stack such that $b$ is the gold
parent of $w$ and $w$ is assigned a head already.
\end{itemize}

The change due to the non-monotonic Reduce action:
\begin{itemize}

   \item $\Delta_{NMD}(\textsc{Shift},c,gold)$: The cost of Shift is decreased by 1 if the gold head of $b$ is $s$.

   \item $\Delta_{NMD}(\textsc{LeftArc},c,gold)$: The cost of Left-Arc is increased by 1 if $s$ is not assigned a head, and the gold head of $s$ is $s_{-1}$.

   \item $\Delta_{NMD}(\textsc{RightArc},c,gold)$ = 0

   \item $\Delta_{NMD}(\textsc{Reduce},c,gold)$ = 0

\end{itemize}

\section{Training Procedure}

Now that we have defined dynamic oracles for the non-monotonic arc-eager
system, we could just plug them in the training algorithm of
\cite{coling2012}.  However, the non monotonic oracles does not give the
parser any incentive to learn not make mistakes that it can potentially recover from
later, even if learning to fix the mistake is harder than learning not to make
it in the first place.

In order to correct for that, we can instead train the parser using both the
monotonic and non-monotonic oracles simultaneously by combining their
judgements: if the non-monotonic oracle assigns several actions a zero-cost,
we prefer to follow those actions that are also assigned a zero-cost by the
monotonic oracle, because these actions can lead to the best outcome without
relying on a repair down the road.

\note{should make nicer!}
\begin{verbatim}

while not state.is_finished {
  feats = extract(state)
  zero_costs =
      O_r(state) intersect O_e(state)
  if zero_costs.empty():
   zero_costs = O_r(state)

  g = predict_from(feats, zero_costs)
  p = predict_from(feats, monotonic)
  if p not in zero_costs {
     update(feats, g, 1)
     update(feats, p, -1)
  }
  transition(state, p)
}
\end{verbatim}
   

% P5
\section{Experiments}

We implemented a standard Arc-Eager parser trained with the averaged Perceptron,
and used it to evaluate the non-monotonic transitions. We tested the transitions on
multiple dependency schemes, to check whether any effects were specific to annotation
contingencies. We also analyse the effect on accuracy by dependency type, and
investigate the impact on error propagation.


\begin{table}[t]
    \small
    \centering
    \begin{tabular}{l|rrrr}
        \hline
                              & Stanford & MALT & CoNLL & ?? \\
        \hline \hline
                              & \multicolumn{4}{c}{Unlabelled Attachment Scores} \\
        \hline
        Baseline              &  00.0  & 00.0 & 00.0 & 00.0   \\
        NM L                  &  00.0  & 00.0 & 00.0 & 00.0   \\
        NM D                  &  00.0  & 00.0 & 00.0 & 00.0   \\
        NM L+D                &  00.0  & 00.0 & 00.0 & 00.0    \\ 
        \hline
                              & \multicolumn{4}{c}{Labelled Attachment Score} \\
        \hline
        Baseline              &  00.0  & 00.0 & 00.0  & 00.0   \\
        NM L                  &  00.0  & 00.0 & 00.0  & 00.0   \\
        NM D                  &  00.0  & 00.0 & 00.0  & 00.0   \\
        NM L+D                &  00.0  & 00.0 & 00.0  & 00.0   \\ 
        \hline
    \end{tabular}
    \caption{\small Effect of non-monotonic transitions with \textbf{standard training}.
             All results averaged across 20 runs with different random seeds. All models
             had $\sigma$=TODO-TODO.\label{tab:standard}}
\end{table}


\subsection{Data and Evaluation}



A train/dev/test split of 02-21 of the \wsj was used for all models.

% P6

\subsection{Parsing model}

We base our experiments on the parser described by \citet{goldberg:12}. We
began by implementing their baseline system, a standard Arc-Eager parser using
an averaged Perceptron learner and the extended feature set described by \citet{zhang:11}.
We follow \citeauthor{goldberg:12} in training all models for 15 iterations,
and in shuffling the sentences before each iteration.
The system is sensitive to the ordering of the sentences, so
the order is shuffled before each iteration and
so accuracies are reported as the average of 20 trials. All experiments had a
standard deviation between TODO and TODO\%.

Labels and moves were learned jointly, resulting in a single model with 88 classes
for the Stanford dependencies. Joint label/move learning proved surprisingly
advantageous, giving 0.4\% additional \uas.

We follow the suggestion of \citet{nivre:squib} to handle root-node dependencies by
adding a dummy token at the \emph{end} of the sentence.
This improves accuracy by delaying root-node decisions, and likely accounts for the
0.2\% extra accuracy of our standard-trained baseline system over the equivalent result
reported by \citet{goldberg:12}.

\subsection{Training strategies}

Section \ref{sec:oracle} summarises a recent innovation in the way transition-based
dependency parsers are trained. This method allows training from states that
result from previous transition errors, which is
important for this work. We therefore evaluate our model with two training strategies.

\textbf{Standard.} 
The parser begins stepping through the sentence, and at each state a small
set of rules determines a single gold-standard transition. The transitions
determine both the label for the instance and the next parser action. No ambiguity
is preserved; the Perceptron's weights are updated even if its prediction was a move
that could lead to the gold-standard dependencies.
This is the baseline method for \citet{goldberg:12}.

\textbf{Goldberg and Nivre.} The parser is allowed to train from states
resulting from previous incorrect transitions. The function determing correctness
is described in Section \ref{sec:oracle}. Another important difference is that
multiple transitions may be regarded as correct, if they do not cause any new
errors. Any prediction within this set will not result in an update to the
Perceptron's weights. This is the method labelled \emph{dynamic+explore}
in \citet{goldberg:12}.

\begin{table}[t]
    \small
    \centering
    \begin{tabular}{l|rrrr}
        \hline
                              & Stanford & MALT & CoNLL & ?? \\
        \hline \hline
                              & \multicolumn{4}{c}{Unlabelled Attachment Scores} \\
        \hline
        Baseline              &  90.0  & 00.0 & 00.0 & 00.0   \\
        NM L                  &  00.0  & 00.0 & 00.0 & 00.0   \\
        NM D                  &  00.0  & 00.0 & 00.0 & 00.0   \\
        NM L+D                &  00.0  & 00.0 & 00.0 & 00.0    \\ 
        \hline
                              & \multicolumn{4}{c}{Labelled Attachment Score} \\
        \hline
        Baseline              &  00.0  & 00.0 & 00.0  & 00.0   \\
        NM L                  &  00.0  & 00.0 & 00.0  & 00.0   \\
        NM D                  &  00.0  & 00.0 & 00.0  & 00.0   \\
        NM L+D                &  00.0  & 00.0 & 00.0  & 00.0   \\ 
        \hline
    \end{tabular}
    \caption{\small Effect of non-monotonic transitions with \textbf{Goldberg and Nivre training}
            All results averaged across 20 runs with different random seeds. All models
             had $\sigma$=0.07-0.08.\label{tab:goldberg}}
\end{table}

\subsection{Transition systems}

\textbf{Baseline.} Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer at mi justo, eu ullamcorper elit. Quisque aliquet massa eget lacus posuere vulputate. Phasellus scelerisque nibh vel nisi dapibus dapibus. Phasellus elementum eleifend neque. Pellentesque non enim urna, in molestie nisi. Sed mattis orci a massa ultricies congue. Suspendisse euismod cursus dolor.

\textbf{NM L.} Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue.

\textbf{NM D.} Non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor.

\textbf{NM L+D.} Non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.


\section{Development Results}
\label{sec:results}

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.


\begin{table}
\centering
\small
\begin{tabular}{lrrrr}
        & Freq. & Base & NM & WD \\
    \hline \hline
    \multicolumn{5}{c}{Left-Arcs} \\
    \hline
    det	& 3350 & 00.0 & +0.0 & 0.0 \\
     nn	& 3208 & 00.0 &	-0.0 & 0.0 \\
  nsubj	& 2691 & 00.0 &	0.0 & 0.0 \\
   amod	& 2428 & 00.0 & 0.0 & 0.0 \\
aux	    & 1229 & 00.0 &	0.0 & 0.0 \\
advmod  & 810  & 00.0 & 0.0 & 0.0 \\
num	    & 749  & 00.0 & 0.0 & 0.0 \\
   poss	& 707  & 00.0 & 0.0 & 0.0 \\
  Other	& 2637 & 00.0 & 0.0 & 0.0 \\
\hline
\multicolumn{5}{c}{Right-Arcs} \\
\hline
pobj	& 3739  & 00.0 & 0.0 & 0.0 \\
prep	& 3498  & 00.0 & 0.0 & 0.0 \\
dobj	& 1568  & 00.0 & 0.0 & 0.0 \\
conj	& 1002  & 00.0 & 0.0 & 0.0 \\
cc	    & 898   & 00.0 & 0.0 & 0.0 \\
number	& 468   & 00.0 & 0.0 & 0.0 \\
 ccomp	& 449   & 00.0 & 0.0 & 0.0 \\
 xcomp	& 438   & 00.0 & 0.0 & 0.0 \\
advmod	& 437   & 00.0 & 0.0 & 0.0 \\
ps	    & 425   & 00.0 & 0.0 & 0.0 \\
dep	    & 406   & 00.0 & 0.0 & 0.0 \\
  Other	& 2549  & 00.0 & 0.0 & 0.0 \\
\hline
All left  & 000 & 00.0 & 0.0 & 0.0  \\
All right & 000 & 00.0 & 0.0 & 0.0  \\
Root    & 1700 & 00.0 & 0.0 & 0.0    \\     
\hline
\end{tabular}
\caption{\small Accuracies for common Stanford labels,
         using G\&N training and standard features.
         \textbf{NM L+D} accuracy is shown by difference from the baseline. The
         WD column shows the difference weighted by the frequency of the label.
     \label{tab:labels}}
\end{table}


\subsection{Accuracy by label}

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.


\subsection{Effect on error propagation}

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.

\begin{table}
    \small
    \centering
    \begin{tabular}{l|rrr|rrr}
        & \multicolumn{3}{c|}{Standard Training} & \multicolumn{3}{c}{G\&N Training}\\
        $E$   & N & Baseline & NM    & N &Baseline & NM    \\
        \hline \hline
        0     &   &          &        &   &       &        \\
        1     &   &          &        &   &       &        \\
        2     &   &          &        &   &       &        \\
        3     &   &          &        &   &       &        \\
        4     &   &          &        &   &       &        \\
        5     &   &          &        &   &       &        \\
        $>5$  &   &          &        &   &       &        \\
        \hline
    \end{tabular}
\caption{\small Likelihood of making a transition error in a state given $E$ previous
         errors with the two training strategies. The system accuracy (NM L+D) is
     shown as a difference from the baseline.\label{tab:errprop}}
\end{table}


\begin{table}
\centering
\begin{tabular}{lrr}
    \hline 
            & \las  & \uas  \\
    \hline \hline 
                & \multicolumn{2}{c}{Standard Features} \\
                \hline
    Baseline    & 00.0 & 00.0 \\
    NM L+D      & 00.0 & 00.0 \\
\hline
                & \multicolumn{2}{c}{Additional features} \\
                \hline
    Baseline    & 00.0 & 00.0 \\
    NM L+D      & 00.0 & 00.0 \\
\end{tabular}
\caption{
    \small
    Impact of the additional features on Stanford \wsj22 using the baseline and
    non-monotonic transition systems.
\label{tab:feats}}
\end{table}


\section{Test Results}

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.


\begin{table}
    \centering
    \small
    \begin{tabular}{l|r|rr|rr}
        \hline 
System  &   $O$      & \multicolumn{2}{c|}{Penn2Malt} & \multicolumn{2}{c}{Stanford} \\
                           &          & \las  & \uas  & \las & \uas \\
        \hline \hline
K\&C 10  & $n^3$ & ---   & 93.00 & ---  & --- \\
Z\&N 11  & $nk$  & 91.8  & 92.9  & 91.9 & 93.5\\
G\&N 12  & $n$   & ---   & ---   & 88.72 & 90.96 \\
        \hline
Baseline    & $n$ &       &       &       &  \\
NM L+D & $n$ &       &       &       &  \\
\hline
    \end{tabular}
    \caption{Test results on \wsj 23. 
         \label{tab:feats}}
\end{table}



% P8
\section{Related Work}


Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.

\subsection{Parse restructuring in psycholinguistics}

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.

\section{Conclusion}

Sed tempus euismod risus et pharetra. Donec consectetur blandit tortor vitae auctor. Sed nec est posuere mauris volutpat laoreet. Nunc placerat orci vitae mauris rhoncus a condimentum arcu vulputate. Ut tincidunt, purus quis ultrices pulvinar, nulla leo scelerisque mauris, sit amet convallis tellus orci eu augue. In tincidunt, dolor rhoncus rhoncus vehicula, magna est laoreet velit, non tincidunt neque nulla vel nulla. Nullam metus enim, blandit eu sodales vitae, dictum vel metus. Praesent nec ornare tellus. Integer aliquet mauris sit amet mi eleifend nec iaculis orci ultrices. Nulla aliquet malesuada porttitor. Sed a erat tortor. Nullam placerat massa sed massa dictum aliquet. Curabitur lobortis dictum tortor, sit amet dignissim erat fringilla sed.

\bibliography{repair}
\bibliographystyle{aclnat}


\end{document}

